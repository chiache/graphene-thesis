\section{Evaluation}
\label{sec:eval}

%%% \begin{figure}
%%% \centering
%%% \includegraphics[width=\linewidth]{plots/readdir.pdf}
%%% \caption{caption}
%%% \label{fig:readdir}
%%% \end{figure}

\begin{comment}
\begin{table}
\scriptsize
\centering
\begin{tabular}{|c||rr|rr|rr||rr||rr|rr|rr|rr||rr|}
\hline
&
\multicolumn{8}{c||}{Unmodified kernel} &
\multicolumn{10}{c|}{Optimized kernel} \\
\hline
&
\multicolumn{2}{c|}{\dentry{}} & \multicolumn{2}{c|}{inode} & \multicolumn{2}{c||}{\dentry{}} & &
& & & & & \multicolumn{2}{c|}{inode} & \multicolumn{2}{c||}{\dentry{}} & & \\
{\bf Path patterns} &
\multicolumn{2}{c|}{lookup} & \multicolumn{2}{c|}{lookup} & \multicolumn{2}{c||}{allocate} & \multicolumn{2}{c||}{\bf lookup time} &
\multicolumn{2}{c|}{\em fastpath} & \multicolumn{2}{c|}{\em slowpath} & \multicolumn{2}{c|}{lookup} & \multicolumn{2}{c||}{allocate} & \multicolumn{2}{c|}{\bf lookup time} \\
& $\upmu$s & +/- & $\upmu$s & +/- & $\upmu$s & +/- & $\upmu$s & +/- & $\upmu$s & +/- & $\upmu$s & +/- & $\upmu$s & +/- & $\upmu$s & +/- & $\upmu$s & +/- \\
\hline
\hline
\multicolumn{19}{|l|}{\bf Warm cache (every lookup cause a cache hit):} \\
\hline
{\tt /X/Y} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 \\
\hline
{\tt /X/Y/Z} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 \\
\hline
{\tt /X/../Y} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00  \\
%\hline
%\scalebox{.8}[1.0]{\tt /proc/*/fd/*} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00  \\
%\hline
%{\tt /X/Y/L} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00  \\
%\hline
%{\tt /L/Y/Z} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00  \\
\hline
\hline
\multicolumn{19}{|l|}{\bf Code cache (every lookup cause a cache miss):} \\
\hline
{\tt /X/Y} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 \\
\hline
{\tt /X/Y/Z} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 \\
\hline
{\tt /X/../Y} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00  \\
%\hline
%\scalebox{.8}[1.0]{\tt /proc/*/fd/*} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00  \\
%\hline
%{\tt /X/Y/L} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00  \\
%\hline
%{\tt /L/Y/Z} & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00 & 000.00 & .00  \\
\hline
\end{tabular}
\caption{Nano-benchmark results of looking up 1000 unique paths of different pattern, in either a cold cache or a warm cache.
{\tt `X'}, {\tt `Y'} and {\tt `Z'} can be any regular file name, and {\tt `L'} is a symbolic link that points to another file (or directory) in the same directory. \fixmetsai{made into stacked bar graph.}}
\label{table:lookup-nanobench}
\end{table}
\end{comment}

This section evaluates our directory cache optimizations,
and seeks to answer the following questions:
\begin{compactenum}
\item How much does each optimization---the lookup fastpath, whole directory caching, and more aggressive negative dentries---improve application performance?  
\item How difficult are the changes to adopt, especially for individual file systems?
%\item Do the changes maintain compatibility with current Linux applications and security modules?
%Does our {\em fastpath} significantly improve directory cache lookup at a cache hit and cause acceptable slow-down at a cache miss?
%\item How much does our solution reduce the synchronization cost for directory cache lookup, and improve multi-core performance and fairness of file system?
%\item How is performance of file systems affected by improving the hit rate at {\tt readdir}, renaming and deletion?
\end{compactenum}


The evaluation includes both 
%{\bf nano-benchmarks} to show impact of our optimization on various directory cache operations,
micro-benchmarks to measure the latency of file system related system calls in best-case and worst-case scenarios,
and a selection of real-world applications to show potential performance boost by our solution in practice.
%Unless otherwise noted, measurements of the optimized kernel include all optimizations.


All experiment results are collected on a Dell Optiplex 790 with a 4-core 3.40 GHz Intel Core i7 CPU, 4GB RAM, and a 250 GB, 7200 RPM ATA disk,
formatted as a journaled {\tt ext4} file system, configured  with a 4096-byte block size.
The OS is Ubuntu 12.04 server, Linux kernel \linuxver{}. 
As illustrated in Figure~\ref{fig:by-version}, more recent kernels have not optimized these data points; we have run these tests 
on unmodified Linux 3.19 and seen no significant changes, but did not have a complete data set at the time of submission.
%We use the latest stable kernel, 3.19, as a baseline for comparison.
All tables include 95\% confidence intervals (``+/-'').

%% dp Meh
%The system includes other standard pseudo file systems, such as {\tt proc}, {\tt sysfs} and {\tt dev}.

\subsection{Single File Lookup}

\begin{comment}
\paragraph{Nano-benchmarks.}
We begin by using a simple test program to make {\tt stat} system calls with 1000 unique paths,
for nano-benchmarking directory cache lookup time in the optimized Linux kernel.
The program is tested in two conditions:
when the directory cache is cold (paths are never checked before, and any lookup will cause a cache miss) and when it is warm (paths are already checked once, and any lookup will cause a cache hit).
Beside canonical paths, we also test several special scenarios:
path as combinations of names and dot-dots,
paths in {\tt proc} file system,
and paths with symbolic links.

To evaluate precisely, we modify the kernel to calculate wall times in the {\tt path\_lookupat} function and
output the accumulated times and numbers of operations through a {\tt proc} file.
We break down the directory cache lookup time into four stackable latencies:
{\em fastpath}, {\em slowpath} (or \dentry{} lookup in the original kernel), inode lookup, and \dentry{} allocation.
Table~\ref{table:lookup-nanobench} shows the results of nano-benchmarking the directory cache lookup.

\fixmetsai{Observation here}
\end{comment}

\begin{table}
\scriptsize
\centering
\begin{tabular}{|l|rr|rrr|}
\hline
Benchmarks & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
& $\upmu$s & +/- & $\upmu$s & +/- & Gain\\
\hline
{\tt stat($path_1$)} & 0.2676 & .000 & 0.2492 & .00 & 6.88 \% \\
\hline
{\tt stat($path_2$)} & 0.5125 & .000 & 0.3764 & .00 & 26.56 \% \\
\hline
%{\tt stat($path_3$)} & 0.6430 & .000 & 0.4104 & .00 & 36.17 \% \\
%\hline
{\tt open($path_1$)} & 0.6735 & .099  & 0.6636 & .13  & 1.47 \% \\
\hline
{\tt open($path_2$)} & 1.3678 & .128  & 0.8049 & .15 &  41.15 \% \\
\hline
%{\tt open($path_3$)} & 2.0240 & .143  & 0.9496 & .14  & 53.08 \% \\
%\hline
\multicolumn{6}{|l|}{\scriptsize\tt $path_1$: /home/foo} \\
\multicolumn{6}{|l|}{{\scriptsize\tt $path_2$: }\scalebox{.8}[1.0]{\scriptsize\tt /usr/lib/gcc/x86\_64-linux-gnu/4.6/include/stddef.h}} \\
%\multicolumn{6}{|l|}{\scriptsize\tt $path_3$: /proc/sys/kernel/printk} \\
\hline
\end{tabular}
\caption{System call {\tt stat} and {\tt open} latency for micro-benchmark ({\tt lat\_syscall} in {\tt LMBench}), based on different paths. Lower is better. This benchmark only evaluates cache hit latency.} 
\label{table:lat_syscall}
\end{table}

\paragraph{Micro-benchmarks.}
We use the LMBench 2.5 UNIX microbenchmark suite~\cite{McVoy:lmbench}
to evaluate latency of path lookup at the system call level.
%The {\tt lat\_syscall} test in {\tt LMBench} simply uses {\tt stat} or {\tt open} system calls repeatedly to access specific paths.
%We exercise the test with three different paths: root of a user home directory ({\tt /home/foo}), a important system header file with long path ({\tt /usr/lib/gcc/x86\_64-linux-gnu/4.6/stddef.h}) and a {\tt proc} file ({\tt /proc/sys/kernel/printk}).
%Generally {\tt lat\_syscall} demonstrates the best-case performance of directory cache lookup.
Table~\ref{table:lat_syscall} shows the latency to 
{\tt stat} and {\tt open} sample paths of different lengths.
Longer paths show a more significant improvement, as the per-component costs in our
optimized kernel are much lower.
Even for the shortest paths, our optimizations yield a few percent improvement;
the best-case improvements are over 40\%.

\begin{table}
\scriptsize
\centering
\subfloat[{\tt chmod} latency]{
\begin{tabular}{|l|rr|rrr|}
\hline
Depth \& size & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel}  \\
& $\upmu$s & +/- & $\upmu$s & +/- & Overhead \\
\hline
single file          & 0.809 & .00 & 0.753 & .000 & 4.0 \% \\
\hline
depth=1,10 files     & 0.828 & .00 & 0.868 & .001 & 21.6 \% \\
\hline
depth=2,$10^2$ files & 0.810 & .00 & 2.477 & .001 & 14.0 \% \\
\hline
depth=3,$10^3$ files & 0.805 & .00 & 16.788 & .002 & 246.0 \% \\
\hline
depth=4,$10^4$ files & 0.787 & .00 & 213.000 & .027 & 29790$\times$ \\
\hline
\end{tabular}
}
\hfill
\subfloat[{\tt rename} latency]{
\begin{tabular}{|l|rr|rrr|}
\hline
Depth \& size & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel}  \\
& $\upmu$s & +/- & $\upmu$s & +/- & Overhead \\
\hline
single file          & 3.72 & .01 & 3.336 & .149 & 4.2 \% \\
\hline
depth=1,10 files     & 4.20 & .02 & 4.375 & .016 & 15.9 \% \\
\hline
depth=2,$10^2$ files & 4.22 & .02 & 11.560 & .026 & 202.1 \% \\
\hline
depth=3,$10^3$ files & 4.22 & .02 & 80.073 & .028 & 1993$\times$ \\
\hline
depth=4,$10^4$ files & 4.20 & .02 & 814.000 & .095 & 21607$\times$ \\
\hline
\end{tabular}
}
\caption{{\tt chmod} / {\tt rename} latency in directories of various depths and sizes. Lower is better.}
\label{table:lat_perm}
\end{table}

To evaluate the overhead of updating directory permissions and changing the directory structure,
we measure {\tt chmod} and {\tt rename} latency.
%We also wrote test cases for 
%creating and removing mount points,
%of different file system types such as {\tt ext4}, {\tt proc}, {\tt sysfs}, {\tt dev}
%and {\tt bind} mounts (logical aliases of other directories).
% {\tt lat\_perm} that makes {\tt chmod} system call on a directory,
%and then revalidates access permissions on every files in the directory by {\tt access} system call.
In our solution, the main factor influencing these overheads are the number of children
in the cache (directory children out-of-cache do not affect performance)
% are the depths and sizes (number of files) of the directory,
so we measured performance on directories with different depths and directory sizes
(Table~\ref{table:lat_perm}).
In general, the cost of a rename or chmod increases dramatically with the number of children,
whereas baseline Linux and ext4 make these constant-time operations.
Even with 10,000 children all in cache, the worst-case latency is under one second.
As a point of reference, the Linux 3.19 source tree includes 51,562 files and directories.
Initial feedback from several Linux file system maintainers indicate that this trade is acceptable
to improve lookup performance~\cite{linux-forum}.

\begin{comment}
\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{|l|rr|rrr|}
\hline
Types & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel}  \\
& $\upmu$s & +/- & $\upmu$s & +/- & Overhead \\
\hline
{\tt ext4} & 0000.00 & .00 & 0000.00 & .00 & 00.0 \% \\
\hline
{\tt proc} & 0000.00 & .00 & 0000.00 & .00 & 00.0 \% \\
\hline
{\tt sysfs} & 0000.00 & .00 & 0000.00 & .00 & 00.0 \% \\
\hline
{\tt dev} & 0000.00 & .00 & 0000.00 & .00 & 00.0 \% \\
\hline
\end{tabular}
\caption{Latency of {\tt mount} / {\tt umount}, for creating and removing mount point based on file system types. Lower is better.}
\label{table:lat_mount}
\end{table}
\end{comment}

%We also evaluate the overhead of Table~\ref{table:lat_mount} shows the result of {\tt lat\_mount}, a {\tt LMBench} test case we extended, to measure latency of {\tt mount} and {\tt umount} system calls.

\begin{table}[t]
\scriptsize
\centering
\subfloat[Warm cache]{
\begin{tabular}{|l|rr|rrr|}
\hline
Applications & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
& s & +/- & s & +/- & Gain\\
\hline
{\tt find -name}
& 0.050 & .00 & 0.029 & .00 & 30.3 \% \\
\hline
\scalebox{.8}[1.0]{\tt tar -xzf linux.tar.gz}
& . & . & 2.732 & 0.00 & 0.5 \% \\
\hline
\scalebox{.8}[1.0]{\tt rm -r linux src}
& . & . & 0.392 & .00 & 12.5 \% \\
\hline
\scalebox{.8}[1.0]{\tt make linux src}
& . & . & 295.738 & .22 & 0.9 \% \\
\hline
\scalebox{.8}[1.0]{\tt make -j4 linux src}
& . & . & 94.158 & .16 & 1.9 \% \\
\hline
{\tt du -s linux src}
& 0.059 & .00 & 0.037 & .00 & 22.6 \% \\
\hline
{\tt updatedb -U /usr}
& 0.021 & .00 & 0.101 & .00 & 29.6 \% \\
\hline
\scalebox{.8}[1.0]{\tt git status linux repo}
& 0.135 & .00 & 0.147 & .00 & 5.8 \% \\
\hline
\scalebox{.8}[1.0]{\tt git status git repo}
& 0.008 & .00 & 0.017 & .00 & 6.4 \% \\
\hline
\scalebox{.8}[1.0]{\tt git diff linux repo}
& 0.052 & .00 &  0.040 & .00 & 4.9 \% \\
\hline
\scalebox{.8}[1.0]{\tt git diff git repo}
& 0.003 & .00 & 0.028 & .00 & 25.7 \% \\
\hline
\end{tabular}
}
\hfill
\subfloat[Cold cache]{
\begin{tabular}{|l|rr|rrr|}
\hline
Applications & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
& s & +/- & s & +/- & Gain\\
\hline
{\tt find -name}
& 3.372 & .00 & 4.858 & .003 & 27.8 \% \\
\hline
\scalebox{.8}[1.0]{\tt tar -xzf linux.tar.gz}
& . & . & 6.718 & .068 & 1.4 \% \\
\hline
\scalebox{.8}[1.0]{\tt rm -r linux src}
&  . & . & 2.877 & .007 & 7.6 \% \\
\hline
\scalebox{.8}[1.0]{\tt make linux src}
&  . & . & 317.381 & .185 & -0.2 \% \\
\hline
\scalebox{.8}[1.0]{\tt make -j4 linux src}
& . & . & 107.857 & .173 & 0.4 \% \\
\hline
\scalebox{.8}[1.0]{\tt du -s linux src}
& 3.517 & .00 & 5.184 & .007 & 5.1 \% \\
\hline
{\tt updatedb -U /usr}
& 5.030 & .01 & 21.616 & .009 & -0.8 \% \\
\hline
\scalebox{.8}[1.0]{\tt git status linux repo}
& 11.46 & .00 & 17.157 & .011 & 0.3 \% \\
\hline
\scalebox{.8}[1.0]{\tt git status git repo} 
& 0.755 & .00 & 0.982 & .001 & 6.4 \% \\
\hline
\scalebox{.8}[1.0]{\tt git diff linux repo} 
& 2.020 & .01 & 3.156 & .001 & 11.1 \% \\
\hline
\scalebox{.8}[1.0]{\tt git diff git repo}
& 0.505 & .00 & 0.472 & .011 & 0.3 \% \\
\hline
\end{tabular}
}
\caption{Execution time in real-world applications bounded by directory cache lookup latency. Lower is better.}
\label{table:lookup-apps}
\end{table}



%%% The cost of changing a directory's permissions ({\tt chmod}) is 12--16\%, depending on how many subdirectories are cached.
%%% We note that changing a file's permission incurs less than 2\% overhead.  We expect that permission changes to 
%%% high-level directories be very infrequent in practice---generally by an administrator; thus, a sub-second cost is still acceptable.
%%% The overhead added to a directory {\tt rename} is roughly 1\%, primarily because {\tt rename} is
%%% already an order of magnitude more expensive than other path-based operations, because of its atomicity requirements.

\paragraph{Space Overhead.}
Our prototype increases the size of a dentry from 192 bytes to 288 bytes.
Because Linux doesn't place any hard limits on dcache size, except extreme under memory pressure, 
it is hard to normalize execution time to account for the space cost.
%We expect sacrificing extra space for performance is a good trade-off for most systems, except perhaps embedded systems.
%We leave quantifying the value of entries
On a typical system, the dcache is tens to hundreds of MB; increasing this by 50\% is likely within an acceptable 
fraction of total system memory.
Alternatively, if one were to bound the total dcache size, this induces
a trade-off between faster hits and fewer hits.  
%We expect that the latency reductions afforded by our lookup optimizations make fewer entries 
%more valuable than more entries in many cases.  
%Alternatively, one could mix dentry sizes, allocating less space for infrequently accessed paths.
We leave exploration of these trade-offs for future work.

\paragraph{Applications.}
The improvement applications see from faster lookup is, of course, proportional 
to the fraction of runtime spent issuing path-based system calls.
We select a range of applications that demonstrate that 
several commonly-used applications will benefit substantially from these optimizations,
and that, in the worst case, any performance harm is minimal.
%Gain for applications depends on whether they has significant ratio of metadata operations,
%and whether they access file systems with temporal locality.
%Based on these criterions, we choose applications
%that are excessively used in practice,
%to determine whether our optimization is truly beneficial for real-world use cases.
The applications we use for benchmarking include:
\begin{compactitem}
\item {\tt find}: search for certain file name in a directory.
\item {\tt tar -xf}: decompress a tarball of directories.
\item {\tt rm -r}: remove whole directories.
\item {\tt make} and {\tt make -jN}: compile a source code tree.
\item {\tt du -s}: Recursively list directory size.
\item {\tt updatedb}: rebuild database of canonical paths for commonly searched file names.
\item {\tt git status} and {\tt git diff}: display status and all unstaged changes in a local git repository.
\end{compactitem}
For each application we test, we evaluate the performance in both cases of cold cache and warm cache.
Table~\ref{table:lookup-apps} shows the results of application performance.
%\fixmedp{Might be good to have a few more cases that change things, like a software install/remove?}
 
Perhaps unsurprisingly, metadata-intensive workloads benefit the most from our optimizations,
such as {\tt find} and {\tt updatedb}, as high 
as 29.6\% faster.  Even 
a recursive remove, which modifies the directory tree one leaf at a time, was faster by up to 12.5\%.
Cases that are dominated by other computations, such as a Linux compile, show little change.
In general, these results affirm that common Linux applications will not be harmed by 
the trade-offs underlying our optimizations, and can benefit substantially.


%\subsection{Synchronization Cost in Directory Cache}

%\fixmetsai{skip this part for now.}


\subsection{Caching Directory Completeness}

\begin{table}[t]
\scriptsize
\centering
%\subfloat[Warm cache]{
\begin{tabular}{|l|rr|rrr|}
\hline
Sizes & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
& s & +/- & s & +/- & Gain\\
\hline
10 & 3.7 & .35 & 2.1 & .15 & 44.1 \% \\
\hline
$10^2$ & 25.1 & .04 & 6.4 & .02 & 74.4 \% \\
\hline
$10^3$ & 268.3 & .10 & 56.1 & .02 & 79.1 \% \\
\hline
$10^4$ & 2759.0 & 1.50 & 545.0 & .05 & 80.3 \% \\
\hline
\end{tabular}
%}
%%% \hfill
%%% \subfloat[Cold cache]{
%%% \begin{tabular}{|l|rr|rrr|}
%%% \hline
%%% Sizes & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
%%% & s & +/- & s & +/- & Gain\\
%%% \hline
%%% 10 & 9,375 & 7,794 & 12,978 & 9,241 & 38.4 \% \\
%%% \hline
%%% $10^2$ & 14,150 & 10,151 & 24,967 & 21,848 & 76.4 \% \\
%%% \hline
%%% $10^3$ & 29,185 & 7,010 & 36,752 & 12,197 & 25.9 \% \\
%%% %\hline
%%% %$10^4$ & 000.00 & .00 & 000.00 & 0.00 & 00.0 \% \\
%%% \hline
%%% \end{tabular}
%%% }
\caption{Latency for {\tt readdir} function call on directories with different sizes. Lower is better.}
\label{table:readdir-microbench}
\end{table}

\paragraph{Micro-benchmarks.}
Table~\ref{table:readdir-microbench} shows the latency 
of a {\tt readdir} microbenchmark with varying directory sizes.
%%% results of improving cache misses for listed directories.
%%% In both original and optimized kernel, the latency of directory listing, using {\tt getdents} or {\tt getdents64} system call,
%%% is relevant to the size (number of files) of the directories.
%%% We create a {\tt LMBench} test case {\tt lat\_readdir} to evaluate average time for listing 1000 directories, using {\tt getdents} system call.
%%% Each target directory in the test case is populated with given number of files before listing it.
The ability to cache {\tt readdir} results improves performance by 44--80\%.
%We omit cold cache data for brevity, which is an extremely noisy function of disk block placement, and shows no clear trend.
Although caching helps more as directories get larger, there is a conventional wisdom that this is only useful for at least 1,024 entries and larger;
this result indicates that there is benefit even for directories with only 10 entries.

%We measured cold cache numbers from this case, i.e., that are from a freshly rebooted system.
%as manually flushing the dcache through {\tt /proc} keeps the disk data cached.
%This data is extremely noisy, and, although our kernel reports a slowdown, both values are still within 
%experimental noise, despite multiple experiments.



\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{|l|rr|rrr|}
\hline
\# of mails & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
& $\upmu$s & +/- & $\upmu$s & +/- & Gain\\
\hline
500 & 2.723 & .234 & 2.501 & .229 & 8.14 \% \\
\hline
1000 & 3.763 & .234 & 3.397 & .216 & 9.72 \% \\
\hline
1500 & 5.007 & .353 & 4.400 & .295 & 12.12 \% \\
\hline
2000 & 6.199 & .320 & 5.465 & .305 & 11.86 \% \\
\hline
2500 & 7.439 & .378 & 6.544 & .349 & 12.03 \% \\
\hline
3000 & 8.596 & .471 & 7.551 & .471 & 12.15 \% \\
\hline
\end{tabular}
\caption{Latency for marking and unmarking mail on {\tt Dovecot} server. Lower is better.}
\label{table:dovecot}
\end{table}

\paragraph{Applications.}
An important example of software that frequently uses {\tt readdir} is an 
IMAP mail server using the MailDir storage format.
%will stores every mail in the mailbox as a file inside a directory, with file name formatted as a combination of timestamp, identifier and flags.
We exercise the Dovecot IMAP server by creating 10 mailboxes for a client.
We use a client script to randomly select messages in different mailboxes and mark them as read, flagged, or unflagged.
Internally, marking a mail causes a file to be renamed, and the directory to be re-read.
To eliminate highly variable network latency, we run network tests on the localhost; 
in a practical deployment, network latency may mask these improvements to the client, but the load of the server will still be reduced.

%will scan the directories of users' mailboxes constantly in order to track the status of all mails in consistent state.
%For evaluation, we create a Dovecot server with 10 mailboxes.
%In each mailbox, we store certain number of mails and use a client-side script written in Python (using {\tt ImapLib} library)
%to randomly mark or unmark 1000 mails with a specific flag.  
%After every time a client has marked or unmark a mail, the mail server will rename the file and rescan the whole directory for a updated list of mails. 


\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{|l|rr|rrr|}
\hline
\# of files & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
& $\upmu$s & +/- & $\upmu$s & +/- & Gain\\
\hline
   10 &    0.33 & 0.0 &   0.28 & 0.0 & 15.7 \% \\
\hline
$10^2$ &   2.31 & 0.0 &   2.11 & 0.0 &  8.5 \% \\
\hline
$10^3$ &  21.33 & 0.0 &  20.34 & 0.1 & 4.7 \% \\
\hline
$10^4$ & 210.72 & 0.8 & 205.67 & 0.5 & 2.4 \% \\
\hline
\end{tabular}
\caption{Average latency of downloading generated directory listing pages from an {\tt Apache} server. Lower is better.}
\label{table:apache}
\end{table}

Table~\ref{table:dovecot} shows the results of {\em Dovecot} mail server; improvements range from 8--12\%.
Commensurate with the {\tt readdir} microbenchmark, larger directories perform better, but even small mailbox sizes benefit from the optimization.
We similarly exercise the Apache web server's ability to generate a file listing using the Apache benchmark (Table~\ref{table:apache}).
These pages are not cached by Apache, but generated dynamically for each request.
Similar to the previous results, the average file listing is 2.4--15.7\% faster. 
Overall, these results show that the readdir caching strategy can reduce server load and improve client response time.

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{|l|rr|rrr|}
\hline
Size of dir & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
& $\upmu$s & +/- & $\upmu$s & +/- & Gain\\
\hline
0 (empty) & 8.198 & .318 & 8.170 & .036 & 0.3 \% \\
\hline
10 & 8.385 & .037 & 8.302 & .035 & 1.0 \% \\
\hline
$10^2$ & 9.905 & .036 & 9.557 & .036 & 3.5 \% \\
\hline
$10^3$ & 13.349 & .036 & 12.351 & .038 & 7.5 \% \\
\hline
$10^4$ & 14.357 & .049 & 13.385 & .041 & 6.7 \% \\
\hline
\end{tabular}
\caption{Latency of using {\tt mkstemp} to create unique file names in directories of different sizes. Lower is better.}
\label{table:lat_mktemp}
\end{table}

\paragraph{File Creation in Completed Directories.}
We measure the latency of creating a secure, randomly-named file in directories of varying size.
We measure 1--7.5\% improvement for the {\tt mkstemp} library, listed in Table~\ref{table:lat_mktemp}. 
Although most applications' execution times are not dominated by secure file creation,
it is a common task for many applications, and of low marginal cost.

%In our solution, if a directory is marked as completed, any consequential file creation in the directory can simply skip disk lookup.
%This optimization generally helps file creation in any directory previously listed, or recently created.
%The most common use case of file creation is {\tt mkstemp}, a GNU Library C function to create unique file names in a given directory.
%We extend {\tt LMBench} with a test case {\tt lat\_mktemp} to evaluate the latency of file creation, within directories of different sizes, as results shown in 

%\fixmetsai{Observation here}

%\fixmetsai{Any applications use mktemp a lot???}
%\fixmetsai{Any applications that populate a empty directory, e.g. git clone, tar -xzf}

\subsection{Negative Caching}

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{|l|rr|rrr|}
\hline
Target & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
& $\upmu$s & +/- & $\upmu$s & +/- & Gain\\
\hline
{\tt rename/creat} & 10.856 & .91 & 10.232 & .01 & 5.75 \% \\
\hline
{\tt open/unlink} & 9.105 & .46 & 8.524 & .02 & 6.38 \% \\
\hline
%& ms & +/- & ms & +/- & Gain\\
%\hline
%${\tt emacsclient} small save & 19.5 & - & 18.5 & - & 5.02 \% \\
%\hline
\end{tabular}
\caption{Latency for microbenchmarks that create and delete the same path in a loop. Lower is better.}
\label{table:neg-dentry-microbench}
\end{table}

%\paragraph{Renaming and Deletion.}

We optimize for the case of repeated creation and deletion of the same file, 
a pattern observed in applications such as text editors.
We measured this pattern with a microbenchmark that renames and creates the same file, and a second
that opens and unlinks the same file.  These cases improved by roughly 6\% (Table~\ref{table:neg-dentry-microbench}).
%We also used the {\tt emacsclient} command line utility to save a few bytes to an empty file.
%Commensurate with the microbenchmark results, the improvement for this case was also 5\%.
Although editors tend to be I/O bound on the user, accelerating this pattern can reduce system load. 


%%% Our solution prevents an unwanted cache miss after any file is renamed or deleted.
%%% To evaluate the improvement of performance based on these two corner cases,
%%% we must use a micro-benchmarking program to create a scenario with intensive loop of file renaming/deletion and recreation.
%%% Despite the overhead of allocation addition negative \dentry{} at renaming/deletion,
%%% the overall performance should should be still improved by skipping disk lookup at file creation.
%%% The result is shown in 

%%% \fixmetsai{Observation here}

\begin{comment}
\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{|l|rr|rrr|}
\hline
Target & \multicolumn{2}{c|}{Unmodified kernel} & \multicolumn{3}{c|}{Optimized kernel} \\
& s & +/- & s & +/- & Gain\\
\hline

\hline
{\tt logrotate} & 000.00 & .00 & 000.00 & 0.00 & 00.0 \% \\
\hline
\end{tabular}
\caption{Latency of applications that constantly {\tt rename/creat} or {\tt unlink/creat} files or directories. Lower is better.}
\label{table:neg-dentry-apps}
\end{table}
\end{comment}

%%% Avoiding cache misses for file creation after renaming or deletion is more likely an optimization beneficial system-wide.
%%% Only very few applications we found have the same scenario of repeatedly renaming/deleting files and recreating them.
%%% Some of the applications are text editors such as {\tt vim} or {\tt emac}.
%%% When a user is editing a file, the editors will create a copy of the file as temporal snapshot, and rename the file to the actual name when user saves the edit.
%%% The scenario of renaming and recreating files happens when user constantly saves and continues editing files.
%%% Another use case is system scripts such as {\tt logrotate} will use renaming as an atomic way of updating files,
%%% and the script will constantly rename existent files or directories as unique new names and recreate them.
%%% Table~\ref{table:neg-dentry-apps} shows the results of the applications we choose, including {\tt emac} and {\tt logrotate}.  

%%% \fixmetsai{Observation here}

%\paragraph{Deep Negative \dentries{}.} 
%%%%%\fixmetsai{skip this part for now.}

\subsection{Code Changes}

\begin{table}[t]
\scriptsize
\centering
%\subfloat[VFS layer]{
\begin{tabular}{|p{1.3in}|r|rr|}
\hline
Source files & \multicolumn{1}{c|}{Original code (LoC)} & \multicolumn{2}{c|}{Our solution (LoC)} \\
\hline
%unifdef -DCONFIG_OPTIMIZE_DCACHE -U CONFIG_OPTIMIZE_DCACHE_CHECK_FULLPATH -DCONFIG_OPTIMIZE_READDIR -UCONFIG_OPTIMIZE_DCACHE_DEBUG -UCONFIG_OPTIMIZE_DCACHE_LOOKUP_NOBARRIER -UCONFIG_OPTIMIZE_DCACHE_REANAME_COPY -DDENTRY_SIGNATURE_BITS=192 -DCONFIG_OPTIMIZE_DCACHE_RENAME_NEGATIVE -DCONFIG_OPTIMIZE_DCACHE_RENAME_NEGATIVE_CREATE -DCONFIG_OPTIMIZE_DCACHE_SIGNATURE_MHASH -DCONFIG_OPTIMIZE_DCACHE_PERM_CACHE_CID -UCONFIG_DENTRY_LOOKUP_TIME -UCONFIG_PATH_LOOKUP_TIME -UCONFIG_DENTRY_ALLOCATE_TIME -UCONFIG_INODE_LOOKUP_TIME -DCONFIG_OPTIMIZE_READDIR_SKIP_LOOKUP  -UCONFIG_OPTIMIZE_DCACHE_SCAN_BACKWARD -UCONFIG_PATH_LOOKUP_BREAKPOINT -UCONFIG_OPTIMIZE_DCACHE_DUMMY  -DCONFIG_DCACHE_WORD_ACCESS -UCONFIG_OPTIMIZE_DCACHE_RENAME_COPY -UCONFIG_OPTIMIZE_DCACHE_STATISTICS -DCONFIG_MHASH_192 -UCONFIG_MHASH_128 -U CONFIG_OPTIMIZE_DCACHE_PROFILE -UCONFIG_OPTIMIZE_DCACHE_SIGNATURE_SIMPLE -UCONFIG_OPTIMIZE_DCACHE_SIGNATURE_JELKINS_OLD -UCONFIG_OPTIMIZE_DCACHE_SIGNATURE_JELKINS_LOOKUP3 -UCONFIG_OPTIMIZE_DCACHE_SIGNATURE_JELKINS_SPOOKY -UCONFIG_OPTIMIZE_DCACHE_SYMLINK_LOOKUP dcache.c > dcache-clean.c
% sloccount fs/dcache-clean.c
{\tt fs/dcache.c} & 1,869 & 2,585 & 38.3 \% \\
\hline
{\tt fs/namei.c} & 2,536 & 3,080 & 21.4 \% \\
\hline
{\tt fs/readdir.c} & 253 & 416 & 64.4 \% \\
\hline
{\tt include/linux/dcache.h} & 238 & 334 & 40.3 \% \\
\hline
AppArmor & 4,404 & 4,408 & 0.0 \% \\
\hline
SELinux & 16,707 & 16,734 & 0.2 \% \\
%\hline
%% not tested, just ignore
%btrfs & 59,683 & 16,734 & 0.2\% \\
\hline
20 Other source or header files &  & +136 &  \\
% security.c +1
% fs/exec.c +1
% fs/internal.h +9
% fs/exec.c +3
% fs/namespace.c +37
% fs/nfsd/ +3
% fs/open.c + 17
% cred.h +2
% fs.h + 7
% namei.h +1
% types.h +1
% capability.c +1
% cred.c + 26
% groups.c + 1
% sys.c +8
%%% security/apparmor/context.c + 4
% security/commoncap.c +1
% security/keys/keyctl.c +2
% security/keys/process_keys.c +4
%%% security/selinux/hooks.c +13
% security/smack/smack_lsm.c +1
% security/tomoyo/securityfs_if.c +1
\hline
%\hline
%Total & 000 & 000 & 00.0 \% \\
%\hline
\end{tabular}
%}
%% \hfill
%% \subfloat[File system drivers]{
%% \begin{tabular}{|p{1.3in}|r|rr|}
%% \hline
%% Source files & \multicolumn{1}{c|}{Original code (LoC)} & \multicolumn{2}{c|}{Our solution (LoC)} \\
%% \hline
%% {\tt ext4} & 000 & 000 & 00.0 \% \\
%% \hline
%% {\tt proc} & 000 & 000 & 00.0 \% \\
%% \hline
%% {\tt sysfs} & 000 & 000 & 00.0 \% \\
%% \hline
%% {\tt dev} & 000 & 000 & 00.0 \% \\
%% \hline
%\end{tabular}
%}
\caption{{\em Lines-of-code} (LoC) in Linux changed, as measured by {\tt sloccount}~\cite{sloccount}.}
\label{table:portability}
\end{table}

In order to measure the difficulty of adoption, we look at the lines of code changed
in our Linux prototype, listed in Table~\ref{table:portability}.
First, we observe that the vast majority of the changes required (about 2,000 LoC)
are localized to the dcache itself ({\tt dcache.c} and {\tt namei.c}).
Second, the low-level file systems we tested didn't require {\em any} changes to use our modified directory caches,
although some file systems that use a custom hash function  would require small changes.
The main impact on other subsystems was actually to the LSMs, which required some changes
to manage CIDs correctly (4--13 LoC out of 4--16 kLoC).
A few other changes were scattered throughout core kernel and VFS files, primarily to manage CIDs.
Thus, the burden of adoption for other kernel subsystems is very minor.


%% difficulty of porting our code to a new Linux kernel, or supporting file system drivers implemented by third-parties.
%% To evaluate portability, we must measure two factors in our solutions: the first is {\em Line-of-code} (LoC) of our patch to the VFS layer in Linux kernels.
%% If the directory cache design in VFS layer is not significantly changed, our patch should be able to apply on new Linux kernels with no pain.
%% Otherwise, our solution may need to be partially reimplemented, based on new interfaces or assumption of kernel codes.
%% The second factor is {\em Line-of-code} of our changes in the code of individual file system drivers.
%% Our solution maintains minimal effort possible to fix file system drivers for making them compatible to the optimized directory cache.
%% The statistic of our solution is show in table~\ref{table:portability}.

\begin{comment}
\subsection{Compatibility}

Demonstrating bug-for-bug compatibility with any large body of software is 
always a challenge, and indicators are noisy.
As a step in this direction, we ran the file system regression test script from the Linux Test Project~\cite{linux-test}
and verified that they pass on our system.
We wrote additional tests that checked that permission handling behaved the same way on AppArmor and SELinux.
This gives us confidence that our prototype's path handling behavior will be transparent to applications.

\end{comment}
