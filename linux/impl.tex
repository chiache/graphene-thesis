\section{Exporting the host ABI}
\label{sec:linux:impl}

\issuedone{1.1.a}{Extend the technical sections}
The initial design of the Linux PAL is based on an unmodified Linux kernel.
By default, a \graphene{} \picoproc{} should run upon
an off-the-shelf Linux kernel as an unprivileged, normal process, with a PAL loaded for exporting \thehostabi{}.
The Linux PAL demonstrates a minimal effort of implementing \thehostabi{}
on a single host, considering Linux is rich with APIs for programming all sorts of applications.
Only two \graphene{} components on the host requires extension or modification of the Linux kernel: a bulk IPC kernel module and a trusted reference monitor.



On a Linux host,
the majority of \hostapis{} are simply wrappers
for similar Linux system calls,
adding on less than 100 LoC on average for each \hostapi{}.
The most complex \hostapis{} on a Linux host are for exception handling, synchronization, and process creation, and each of these \hostapis{} requires multiple system calls and roughly 500--800 LoC in PAL.
For example, process creation (i.e., \palcall{ProcessCreate}) requires 
both the \syscall{vfork} and \syscall{execve} system calls
for creating a clean
application instance, and would be more efficiently
implemented inside the Linux kernel.
Finally, the other major PAL components are an ELF loader (2 kLoC),
Linux kernel and PAL headers (800 LoC),
and internal support code providing functions like \funcname{malloc} and \funcname{memcpy} (2.3 kLoC).
Table~\ref{tab:libos:loc} lists the lines of code of each components of the \graphene{} architecture on a Linux host.


A PAL developer can use the Linux PAL to estimate the baseline of \thehostabi{} development effort on a full-featured host OS .
About half of the Linux PAL code turns out to be
mostly generic to every host OSes
and thus fully reusable for each PAL.
The generic parts include
the ELF loader, PAL headers, and internal support code, adding up to \roughly{}6,000 LoC.
The other half of the Linux PAL are host-dependent code containing mainly wrappers for Linux system calls.
%and has to be replaced with
%host-dependent code.
If the targeted host OS has exported a UNIX or POSIX-like API,
porting the host-dependent code is mostly straightforward.
For example, a follow-up experiment of developing a FreeBSD PAL finds most of the Linux PAL code to be highly portable.





\begin{table}[t!b!]
\footnotesize
\centering
\begin{tabular}{|l|rr|}
\hline
{\bf Component} & {\bf Lines} & ({\bf \% Changed})\\
\hline
GNU Library C ({\tt libc}, {\tt ld}, {\tt libdl}, {\tt libpthread}) & \libclines{} & $0.07\%$ \\
\hline
Linux Library OS (\thelibos{}) & 31,112 & \\
Linux PAL & 12,529 & \\
%Extra code for Linux SGX host \pal{} & 9.354 & \\
% updated by Chia-Che on Oct. 10, 2013
\hline
%Storage Server & \fixmedp{XX} & \\
Reference monitor bootstrapper & \reflines{} & \\
Linux kernel reference monitor module ({\tt /dev/graphene}) & \sandboxmodlines{} & \\
Linux kernel IPC module ({\tt /dev/gipc}) & \gipclines{} & \\
\hline
\end{tabular}
\caption{Lines of code written or changed to develop the whole \graphene{} architecture on a Linux hosts.  The application and other dynamically-loaded libraries are unmodified.}
\label{tab:libos:loc}
\end{table}


%\paragraph{Alternative \pal{} Ports.}
%We prove the platform independence of \graphene{}
%by porting \pal{} to \emph{FreeBSD}, \emph{OSX} and \emph{Windows}.
%With the alternative host \pal{}, unmodified Linux binaries,
%along with {\tt glibc} and {libLinux},
%can be transparently run on the host.
%For FreeBSD,
%only 1.2 kLoC of the host \pal{} code need to be rewritten,
%which are significantly less than FreeBSD Linux compatibility module (10.8 kLoC).
%\pal{} components including ELF loader and internal support code can be shared by any \pal{} ports.

%\fix{We leave host \pal{} ports to non-unix OSes like Windows as future work,
%but previous works~\cite{porter11drawbridge,baumann13bascule} have already shown it feasible.}




%% * most calls are a wrapper, \fixmedp{XX} LoC on average.
%% * Exception handling, sync, and process creation were harder (500-800 LoC each).  Process creation requires a clean instance (vfork+exec), would be simpler to implement in kernel.
%% * Other major components: ELF loader (2kLoC), headers(800 LoC), internal support code (2300 LoC)


%\fixmedp{Chia-Che, update LoC table}


\subsection{Implementation details}

The rest of this section
will discuss a few PAL ABI abstractions that are particularly challenging on a Linux host.
Similar challenges have been presented %have been observed for implementing \thehostabi{}
on other alternative host kernels such as FreeBSD, OS X, and Windows.
%except that other kernels might introduce additional host-specific challenges.


\paragraph{Bootstrapping a \picoproc{}.}
The Linux PAL works as a run-time loader to the \graphene{} \libos{} (\thelibos{}). % and Linux applications.
First, for the dynamic loading of \thelibos{},
the Linux PAL contains an ELF loader, similar to the functionality of a \libc{} loader (\code{ld.so}),
to map the \thelibos{} binary into a \picoproc{} and resolve the addresses of \hostapis{}.
Second, the Linux PAL constructs a process control block (PCB),
providing information
about the \picoproc{} and the host platform.
For example, a member of the PCB exposes the basic CPU information (e.g., model name and number of cores) to \thelibos{}, 
for implementing the \code{cpuinfo} entry of the \code{proc} file system
as a \libos{} abstraction.
Finally,
the Linux PAL populates the stack with program augments and environment variables passed from the command line and switches to \thelibos{}.



\paragraph{RPC streams.}
Three Linux abstractions are candidates for implementing RPC streams: pipes, UNIX domain socket, and loopback network sockets (bound at \code{127.0.0.1}).
Creation of loopback network sockets
is restricted by 65,535 ports which can be used to bind a socket on a network interface. % putting an upper bound on the total number of RPC connections on the same host.
The initial design of the Linux PAL uses pipes to implement RPC streams,
but a later version switches to UNIX domain sockets.
Although both pipes and UNIX domain sockets are viable options,
different performance patterns are expected on these two Linux abstractions.
The general expectation is that a pipe has much lower latency for sending small messages, whereas a UNIX domain socket has much higher throughput for sending large chunks of payloads.
According to a micro-benchmark result of \lmbench{}, on Linux 4.10 kernel,
the latencies of sending one byte over a pipe and a UNIX domain socket are \roughly{}2.2 \us{} and \roughly{}3.5--4.5 \us{}, respectively.
As for throughput, when sending a 10MB buffer, the bandwidth of a UNIX domain socket can reach \roughly{}12 GB/s, whereas the bandwidth of a pipe is only \roughly{}5 GB/s (for reference, the bandwidth of a loopback network socket is \roughly{}7.5 GB/s), at less than half of the transfer rate of a UNIX domain socket.
Based on the performance patterns described above, the latest design of the Linux PAL chooses UNIX domain socket
for prioritizing the RPC throughput of sending large messages, particularly for migrating a snapshot of a forking process.





\paragraph{Exception handling.}
The Linux PAL can receive hardware exceptions (e.g., memory faults, illegal instructions, divide-by-zero) from an application, \thelibos{}, or the PAL itself.
A Linux kernel delivers all hardware exceptions to the user space
as signals.
If an exception is external to the Linux PAL (from an application or \thelibos{}),
the registered signal handler of the Linux PAL simply calls
a guest exception handler assigned by \thelibos{} using \palcall{ExceptionSetHandler}.
The Linux PAL also creates an exception object, either \funcname{malloc}'ed or allocated from the stack,
to pass the exception type and the interrupted context
to the guest exception handler.
Otherwise, if an exception happens internally,
the Linux PAL cannot deliver the exception to the guest exception handler because \thelibos{} does not know how to recover from the exception (the execution inside the Linux PAL is transparent to \thelibos{}).
Unless an exception
happens during the initialization,
it must be triggered inside a \hostapi{} made by \thelibos{} or the application.
To recover from an internal exception,
the Linux PAL stores a piece of recovery information on stack
at the entry of each \hostapi{}.
The PAL signal handler
identifies the internal exception by comparing the faulting address
to the mapping address of the Linux PAL,
discovers the recovery information from the stack,
rolls back the \hostapi{},
and returns as a failed \hostapi{}.



The PAL signal handler must avoid further triggering any hardware exceptions from the handler itself, or it can cause a {\bf double fault}.
\graphene{} ensures that the signal handler is carefully developed
so that no memory faults or other exceptions
can be caused by defects in the handler code.
An unrecoverable case is corruption of a user stack,
since the Linux kernel needs to dump the signal number and interrupted context on the stack
before calling the PAL signal handler.
The Linux PAL can avoid this case by assigning an alternative stack,
or just kill the \picoproc{} when a double fault happens.







 



%\paragraph{Synchronization.} Perhaps surprisingly, implementing Linux
%synchronization appears substantially easier than Windows synchronization, 
%as {\tt libLinux} did not require all of the various
%synchronization ABIs provided by Drawbridge.
%We believe the reason for this is that Linux has consolidated 
%all user-level synchronization primitives to use futexes~\cite{franke02futex},
%which are essentially kernel-level wait queues.
%%In Windows parlance, this is simply an Event associated with a virtual address.
%%Thus, our effort implementing synchronization was relatively straightforward.


\paragraph{Process creation.}
Because the Linux PAL is designed as a runtime loader,
a new, clean \picoproc{} can simply be created by calling \syscall{vfork} and \syscall{execve} with the Linux PAL as the executable.
Within an application instance, the procedures for launching the first \picoproc{} and the consecutive ones
are mostly identical,
except a consecutive \picoproc{} is launched with a heritage of a global PAL data structure (containing a \graphene{} instance ID and a UNIX domain socket prefix), a broadcast stream,
and an unnamed UNIX domain socket as a RPC stream to its parent.
The Linux PAL keeps the other stream handles private to a \picoproc{}
by marking the underlying file descriptors
as \code{CLOEXEC} (close upon \syscall{execve}).
No page needs to be shared among \picoprocs{}.


A key challenge to implementing process creation in the Linux PAL
is to reduce the overhead
of initializing a new \picoproc{}.
The elapsed time of process creation---from an existing \thelibos{} instance calling \palcall{ProcessCreate} to a new \thelibos{} instance starting to initialize---contributes a major part of the \syscall{fork} latency overhead
in \graphene{}.
There were several attempts of optimizing the process creation mechanism
in the Linux PAL.
The current design leverages \syscall{vfork} and \syscall{execve},
but caches the result of relocating symbols in the Linux PAL loader to reduce consecutive \picoproc{} initialization time.
A previous design
uses \syscall{fork} to snapshot a \picoproc{} with a fully-initialized PAL,
and create \picoprocs{} ahead of time.
The preforking design shows higher latency than the current design
due to the IPC overhead for coordinating preforked \picoprocs{}.




\paragraph{Bulk IPC.}
The Linux PAL provides a \code{gipc} kernel module for
transferring the physical pages of a large chunk of memory to another \picoproc{}. 
The \code{gipc} module exports a miscellaneous device, \code{/dec/gipc}, for committing physical pages from a \picoproc{} to an in-kernel store and mapping physical pages copy-on-write in another \picoproc{}.
When \code{gipc} receives
a request of committing a range of memory, it pins all the physical pages within the range,
updates the page table to mark the pages copy-on-write,
and awaits requests of mapping the pages to another \picoproc{}.
Each physical page store has a limited number of slots for queuing physical pages,
so a \picoproc{} can block during committing a range of memory
if the physical page store is full.


The bulk IPC abstraction honors the sandbox boundary. An in-kernel physical page store cannot be shared across sandboxes.
Any \picoprocs{} in a sandbox
can access the same physical page store using an unique store ID;
\picoprocs{} in different sandboxes can
use the same store ID but will open different physical page stores.


