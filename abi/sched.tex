\subsection{Threading}
\label{sec:abi:thread}

\issuedone{1.2.a}{discuss resource management at host level (threading)}
Threading in the host ABI includes two kinds of features:
one is to create a thread, for scheduling the CPU resources; the other is to synchronize or coordinate among multiple threads,
using several scheduling primitives programmable by applications.
Both Linux and POSIX have defined a rich threading API, with various thread creation options
and scheduling primitives.
\graphene{} simplifies the host ABI
to include only a small set of threading features.
%for each host to implement.



\subsubsection*{Creating a thread}



Unlike a user thread in Linux or POSIX (i.e., a ``pthread''),
a guest thread created by the host ABI,
using \palcall{ThreadCreate},
is simply a new context which starts at a function.
The host ABI
moves all the thread creation options,
such as thread-local storage,
to an initialization function called after the thread creation,
inside the library OS.
The purpose of \palcall{ThreadCreate}
is simple: it creates a ``kernel thread'' in the host,
which can be scheduled by the host scheduler,
to run application code on another CPU core.
The \graphene{} library OS does not implement its own scheduler.


\begin{paldef}
HANDLE ThreadCreate (void (*start) (void *), void *arg);
void ThreadExit (void);
\end{paldef}


The host ABI does not implement thread exiting notifications.
When a guest thread is terminated using \palcall{ThreadExit}, the PAL destroys the correspondent kernel thread
without notifying other threads.
The design counts on the library OS to implement the Linux-style notifications,
including
sending a \code{SIGCHLD} signal,
or triggering a parent-wakening futex.
% assigned by a child thread ID field given to  




\subsubsection*{Scheduler}




Besides thread creation,
the host ABI needs scheduling features to interrupt a thread execution,
or yield the CPU.
The concerns for including scheduling features
in the host ABI
is two-fold.
The first concern is regarding compatibility;
an application may be stuck in a busy-waiting loop, if the library OS lacks the ability to interrupt the execution when certain events occur.
Specifically, \palcall{ThreadInterrupt} interrupts a thread using a thread handle
returned by \palcall{ThreadCreat}.
The second concern is regarding the CPU occupancy;
if a thread does not forfeit the CPU when it stops making progress,
the CPU can be wasted being idle.
\palcall{ThreadDelay} and \palcall{ThreadYield}
delays the current thread, until a period of time has passed, or the host scheduler re-schedules the thread to a CPU core.



\begin{paldef}
bool ThreadDelay     (ulong delay_microsec);
void ThreadYield     (void);
void ThreadInterrupt (HANDLE thread_handle);
\end{paldef}



Similar to memory management, the host ABI also delegates scheduling to the hosts.
As the design and implementation of the scheduling policies
differ from host to host,
the library OS cannot implement all the Linux scheduling API and policies in the user space,
unless the host ABI exposes a wide interface
for configuring scheduling options.
In fact, most of the scheduling options in Linux does not impact
the functionality of an application;
for example, without configuring the scheduling priority of threads,
the application will still work,
with potential performance penalty.
The only exception is when an application contain
producer and consumer threads,
which continues to poll a work queue;
in this producer-consumer scenario,
the threads must run on different CPU cores, to prevent deadlocks.
We propose adding a function called \palcall{ThreadSetCPUInfinity} to the host ABI,
to support binding a thread to CPU cores:

\begin{paldef}
bool ThreadSetCPUInfinity (uint cpu_nums[], uint count);
\end{paldef}


\subsubsection*{Scheduling primitives}


The host ABI also includes scheduling primitives, such as locking, for synchronizing the execution
of several threads running in parallel.
The host ABI must provide scheduling primitives,
because locking cannot be reliably implemented in the user space;
user-space locking
cannot prevent a thread from being interrupted inside a critical section,
and thus 
causing the application to deadlock.
Also, user-space locking must be implemented using the compare-and-exchange (\code{CMPXCHG}) instructions, which may not be available on some architectures.
Therefore, the host ABI
can encapsulate the scheduling primitives on different hosts.



\begin{paldef}
HANDLE SemaphoreCreate  (uint max_count, uint inital);
void   SemaphoreRelease (HANDLE semaphore_handle);
void   SemaphoreDestroy (HANDLE semaphore_handle);
HANDLE SynchronizationEventCreate (void);
HANDLE NotificationEventCreate    (void);
void EventSet     (HANDLE event_handle);
void EventDestroy (HANDLE event_handle);
\end{paldef}



\graphene{} inherit two types of scheduling primitives
from the \drawbridge{} host ABI:
semaphores and events.
The events include synchronization events and notification events;
the former wakens a sleeping consumer thread
from a producer; the latter notifies the occurrence of an event and prevent further blocking.
Both scheduling primitives share lots of resemblance with the Windows API;
however, we show that these scheduling primitives are also portable on other hosts, including Linux, BSD, and SGX, using futexes or similar locking mechanisms.









\subsubsection*{Polling handles}


The host ABI allows the library OS to poll one or several handles
at the same time.
The possible handles to poll include semaphores, synchronization or notification events,
or I/O streams.
Therefore, the host ABI
introduces a function, \palcall{ObjectsWait}, similar to \syscall{poll} in POSIX,
with a timeout option.



\begin{paldef}
HANDLE ObjectsWait    (HANDLE *handles, uint nhandles,
                       ulong timeout);
HANDLE StreamGetEvent (HANDLE stream_handle,
                       uint event_flags);
\end{paldef}


A challenge to implementing polling
to differentiate the I/O events which can occur on a stream.
When blocking on a network or RPC stream, an application needs to be notified
about three types of events:
establishment of the connection (i.e., becoming writable), arrival of messages (i.e., becoming readable), and shut-down of the connection;
thus, the \graphene{} host ABI introduces a new function,
\palcall{StreamGetEvent},
to generate a handle identifying these I/O events of a stream,
and can be polled by \palcall{ObjectsWait}.
The design also keeps the definition of \palcall{ObjectsWait} simple:
\palcall{ObjectsWait} should do nothing more than blocking on an array of handles,
until one of the handles
is wakened, or the call timeouts.


\subsubsection*{Thread-local storage}


Most OSes requires certain form of thread-local storage (TLS),
either to store thread-private variables, or to maintain thread control blocks (TCBs).
Because TLS can be frequently accessed
in an application,
it would be inefficient to constantly
enter the kernel for retrieving the TLS data.
A common design is to occupy one of the thread-private registers
to store a pointer to the TLS;
on x86, either the FS or GS register is commonly used to reference the TLS.




\begin{paldef}
void SegmentRegSet (uint register, ulong *value);
\end{paldef}



\issuedone{1.3.e}{discuss the FS/GS limitation}
The host ABI includes a function, \palcall{SegmentRegSet}, to set the FS/GS registers.
The implementation of \palcall{SegmentRegSet}
depends the host system interfaces. 
By default, reading or writing the value of the FS/GS registers is a privileged operation
which can only be performed in ring 0.
On Linux or BSD, the FS register is commonly used for bookkeeping in the standard C library;
thus, the Linux or BSD system calls naturally include the feature of reading or writing the FS/GS registers in the kernel space (although only the FS register is used in the user space).
However, on other hosts, especially Windows and OSX,
we observe the case where changing the FS/GS registers is considered unnecessary and dangerous to the kernel.
These hosts periodically reset the value of the FS/GS registers,
preventing the host ABI to assign the TLS permanently.


The primary challenge
to implementing TLS for Linux applications
is that an executable can hard-code the references to the FS register in its binary.
Because a Linux executable is usually {\em unrelocatable},
it can access a thread-private pointer by simply reading or writing to a specific offset
to the FS register.
To support these applications,
a host must populate a valid TCB at the address pointed by the FS register;
if the host forbids setting the FS register,
the library OS cannot support TLS for an executable using thread-private pointers.

%On these hosts, the compatibility for TLS usage in applications is partially sacrificed.



\subsection{Processes}
\label{sec:abi:proc}


\graphene{} implements a distributed OS model for multi-process applications.
Each process in \graphene{} has a library OS instance;
Multiple library OS instances in a multi-process application must work together
to present a single OS view,
same as running in a native Linux.
Therefore, the host ABI does not require copy-on-writing forking,
but simply creation of a clean process instantiating the library OS.




\begin{paldef}
HANDLE ProcessCreate (const char *executable,
                      const char *manifest,
                      const char **args, uint flags);
\end{paldef}

When creating a process, the host ABI, as \palcall{ProcessCreate}, takes a URI of the executable to run in the new process,
together with a manifest file, which specifies the user policy.
Once a new process is created, the library OS can be initialized
and migrate the library OS and application state from the parent process;
after migration, the new process can resume execution from the point of checkpointing,
as a forked process.
To the hosts, the processes in an application share nothing
but the I/O streams opened by multiple processes.


\palcall{ProcessCreate} returns a process handle.
To bootstrap the inter-process communication, a process handle also works as an unnamed RPC stream connecting the parent and child processes.
The initialization of a library OS instance uses the RPC stream
to retrieve namespace coordination information, such as how to locate the namespace leaders.
The PRC stream is also used to send process migration data
from the parent process, to implement forking.






\subsubsection*{Sharing a handle}




Due to the statelessness of stream handles,
a library OS can cleanly migrate its state to a new process, and recreate all the stream handles previously opened in the process.
Unfortunately, not all I/O streams can be recreated
in a new process, due to the limitations of host system interfaces;
in most hosts, a network connection is only identified by one handle or file descriptor,
and can only be shared when the handle or file descriptor
is inherited by a child process.
The same inheritance feature also has to be implemented
inside the library OS, to support Linux applications.
Since every process created by \palcall{ProcessCreate} is a clean \graphene{} process with freshly-initialized library OS state,
the library OS needs extra functions in the host ABI
to inherit handles instead of data from the parent process.


\begin{paldef}
void   RpcSendHandle (HANDLE rpc_handle, HANDLE cargo);
HANDLE RpcRecvHandle (HANDLE rpc_handle);
\end{paldef}



\graphene{} introduces two functions to the host ABI for handling sharing: \palcall{RpcSendHandle} and \palcall{RpcRecvHandle}, to send a handle over a connected RPC stream.
The design is motivated by the Linux file descriptor sharing feature of UNIX domain sockets.
\palcall{RpcSendHandle} will migrate the state of a stream handle, created by \palcall{StreamOpen},
over a RPC stream, to the receiving process, which calls \palcall{RpcRecvHandle}.
\palcall{RpcSendHandle} will grant the receiving process the permission, to share the stream handle with the sending process.


\subsubsection*{Bulk IPC (physical memory store)}


The \graphene{} design which migrates and coordinates process states over RPC stream
can potentially cause significant overhead on an multi-process application;
especially, the RPC-based migration will slow down the latency of copy-on-write forking.
the key overhead
is caused by copying large chunk of memory
across processes, without the help of a host to share the physical memory.

\begin{paldef}
HANDLE PhysicalMemoryStore  (uint index);
ulong  PhysicalMemoryCommit (HANDLE store_handle,
                             void *addr, ulong size);
void * PhysicalMemoryMap    (HANDLE store_handle,
                             void *addr, ulong size,
                             uint protection);
\end{paldef}


\graphene{} introduces a Bulk IPC feature in the host ABI, to effectively reduce the overhead on implementing forking.
The abstraction provided by the Bulk IPC feature
is a physical memory store (created by \palcall{PhysicalMemoryStore});
during process migration, the parent process
can send application memory to the physical memory store (using \palcall{PhysicalMemoryCommit}), which will keep a snapshot of the sent memory, using copy-on-write.
The child process can then attach to the physical memory store,
and map the memory snapshot into its memory (using \palcall{PhysicalMemoryMap}), also backed by copy-on-write.
As a result, the Bulk IPC feature can large reduce the amount of physical memory copied during process migration,
and thus optimize the latency of forking.




\subsubsection*{Sandboxing}


\begin{paldef}
bool SandboxSetPolicy (const char *sandbox_manifest,
                       bool sandbox_rpc);
\end{paldef}



