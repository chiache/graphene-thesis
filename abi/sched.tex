\papersubsection{CPU scheduling}
\label{sec:abi:thread}

\issuedone{1.2.a}{Discuss resource management at host level (threading)}
The host ABI for CPU scheduling includes two abstractions:
creation of user threads, and scheduling primitives
for inter-thread synchronization and coordination.
%Both Linux and POSIX have defined a rich threading API, with various thread creation options
%and complex scheduling primitives.
%\graphene{} simplifies the host ABI
%by include only a small set of basic threading features.
%for each host to implement.
Threading in the host OS requires a CPU scheduler
to dynamically assign a non-blocking thread to an idle CPU core, until next epoch for scheduling.
The host OS usually implements one or several scheduling algorithms and also defines APIs for applications
to configure scheduler parameters.
The design of scheduling algorithms and
APIs is mostly idiosyncratic to each host OS,
and is difficult to find any common ground to define unified host ABIs that are portable on every host OSes.

%however, a host scheduler does have to ensure every thread to follow its expected behaviors,
%regardless of the scheduling algorithm.
%For the least, a host scheduler should avoid completely starving one of the living threads,
%so that the guest can make progress as expected.
%Other CPU scheduling criteria such as fairness, throughput, and CPU utilization
%are still critical to the application performance, but the host scheduler is responsible of improving these criteria.


As a compromise, \graphene{} focuses on defining host ABIs for CPU scheduling features
essential to application usability.
For instance, applications may depend on multiple threads to execute concurrently, either on different CPU cores or on the same CPU core with a time-sharing model.
Scheduling algorithms in the host OSes are generally expected to satisfy certain criteria, such as fairness, throughput, and reasonable CPU utilization.
As long as the host OSes have chosen
a general-purpose, maturely-implemented scheduling algorithm,
\thehostabi{} can omit features
for configuring scheduler parameters. 






\papersubsubsection{Creating or terminating a thread}



%Unlike a user thread in Linux or POSIX (i.e., a ``pthread''),
%a guest thread created by the host ABI,
%using \palcall{ThreadCreate},
%is simply a new context which starts at a function.
%The host ABI
%moves all the thread creation options,
%such as thread-local storage,
%to an initialization function called after the thread creation,
%inside the library OS.
%The purpose of \palcall{ThreadCreate}
%is simple: it creates a ``kernel thread'' in the host,
%which can be scheduled by the host scheduler,
%to run application code on another CPU core.
%The \graphene{} library OS does not implement its own scheduler.


\begin{paldef}
HANDLE ThreadCreate (void (*start) (void *), void *param);
\end{paldef}


\palcall{ThreadCreate} creates a host-level thread to be schedule by the host OS.
The parameters specify the initial state of a new thread, including the function to start thread execution and the parameter being passed to the function.
As soon as \palcall{ThreadCreate} successfully returns,
the caller thread and the new thread can be both scheduled by the host OS. %should both be live in the host OS.
%and scheduled to run in arbitrary order.
\palcall{ThreadCreate} returns a thread handle to reference the new thread in the caller.




To improve portability, \graphene{} simplifies
the definition of \palcall{ThreadCreate} in several ways.
First, \palcall{ThreadCreate} does not accept additional parameter for specifying
an initial stack of the new thread.
The simplification helps porting \palcall{ThreadCreate}
on a host where a new thread cannot start
on an arbitrarily-assigned stack.
For instance, an \sgx{} enclave statically defines the stack address of each thread
to prevent the host OS from manipulating
enclave thread execution.
On most host OSes, a new thread created by \palcall{ThreadCreate} starts on a fixed-size stack, but the \libos{} can easily swap the stack with a much larger one.
Second, \palcall{ThreadCreate} takes no creation options
except a starting function and a parameter.
Every threads created by \palcall{ThreadCreate} should look identical to the host OS,
in order to keep the abstraction maximally portable
on various host options.



\begin{paldef}
void ThreadExit (void);
\end{paldef}

\palcall{ThreadExit} terminates the current thread. The \hostapi{} takes no argument, and should never return if it succeeds.
The purpose of \palcall{ThreadExit} is to free the resources allocated in the host OS
for the current thread, including the initial stack.




%The host ABI does not implement thread exiting notifications.
%When a guest thread is terminated using \palcall{ThreadExit}, the PAL destroys the correspondent kernel thread
%without notifying other threads.
%The design counts on the library OS to implement the Linux-style notifications,
%including
%sending a \code{SIGCHLD} signal,
%or triggering a parent-wakening futex.
% assigned by a child thread ID field given to  


\papersubsubsection{Scheduling a thread}

\Thehostabi{} defines several calls to interrupt a thread, either blocking or running,
or to allow a running thread to voluntarily give up CPU resources.
The purpose of these scheduling APIs is to prevent a thread from busily waiting for a specific condition,
such as a variable being set to specific value or a certain time in the future.
Busy-waiting wastes CPU cycles,
and can potentially block application execution
if the host OS
has no enough CPU cores to schedule each thread or does not
implement a time-slicing scheduling algorithm (e.g., round-robin).
%Although the guest delegates scheduling to the host scheduler, the guest can proactively request for scheduling to improve CPU throughput.



%Besides thread creation,
%the host ABI needs scheduling features to interrupt a thread execution,
%or yield the CPU.
%The concerns for including scheduling features
%in the host ABI
%is two-fold.
%The first concern is regarding compatibility;
%an application may be stuck in a busy-waiting loop, if the library OS lacks the ability to interrupt the execution when certain events occur.
%Specifically, \palcall{ThreadInterrupt} interrupts a thread using a thread handle
%returned by \palcall{ThreadCreat}.
%The second concern is regarding the CPU occupancy;
%if a thread does not forfeit the CPU when it stops making progress,
%the CPU can be wasted being idle.
%\palcall{ThreadDelay} and \palcall{ThreadYield}
%delays the current thread, until a period of time has passed, or the host scheduler re-schedules the thread to a CPU core.



\begin{paldef}
u64  ThreadDelay (u64 delay_microsec);
void ThreadYield (void);
\end{paldef}


\fixmedp{Define the semantics a bit more. When you do a delay, presumably wait at least that long; are theere any cases you can return early (e.g., EINTR)?}
\palcall{ThreadDelay} and \palcall{ThreadYield} suspend the current thread for rescheduling in the host OS.
\palcall{ThreadDelay} suspends the current thread
for the given timespan (\palkeyword{delay_microsec}, in microseconds).
If the thread is suspended successfully and rescheduled after expiration of the specified period,
\palcall{ThreadDelay} returns zero after resuming thread execution.
If the thread is rescheduled prematurely, due to interruption of other threads (using \palcall{ThreadInterrupt}),
\palcall{ThreadDelay} returns the remaining suspension time in microseconds.


\fixmedp{When you do a yield, under what conditions are you rescheduled?}
\palcall{ThreadYield} simply yields the execution of current thread, and the thread can be rescheduled immediately by the host OS.
By calling \palcall{ThreadYield}, a thread can requests for rescheduling when it expects
to wait for certain conditions.
When a thread calls \palcall{ThreadYield}, the host scheduler will suspend the current time slice of the thread,
and rerun the scheduling algorithm
to select a runnable thread (can be the same thread if there is no other competitor).



\begin{paldef}
void ThreadInterrupt (HANDLE thread_handle);
\end{paldef}



Introduced by \graphene{},
\palcall{ThreadInterrupt} interrupts a thread and forces the thread to enter an exception handler immediately.
The definition of \palcall{ThreadInterrupt} has two primary purposes.
First, \palcall{ThreadInterrupt} can interrupt a suspended thread, and force the thread to resume execution immediately.
Second, \palcall{ThreadInterrupt} can interrupt a running thread from an infinite waiting loop.
%so that the thread can instantaneously respond to a sudden event.
Without \palcall{ThreadInterrupt},
a running thread can only detect events at a certain ``checkpoint'' in the \libos{}.


The three abstractions defined for suspension and rescheduling
commonly exist on most host OSes, including Linux, \win{}, and \osx{}.
System APIs similar to
\palcall{ThreadDelay} and \palcall{ThreadYield}
can be found on all host OSes,
with slight but mitigable definition differences.
\palcall{ThreadInterrupt} can be implemented with signaling on a POSIX-style host OS,
or similar inter-thread communication
mechanisms on other OSes.







\paragraph{Scheduler parameters.}
\Thehostabi{} currently contains no APIs for the \libos{} to configure scheduler parameters in the host OS.
%the host scheduler.
Generally, Linux and other OSes allow applications to configure scheduling parameters,
such as scheduling priorities and policies,
to improve CPU utilization.
For simplicity,
\thehostabi{} delegates scheduling to the host scheduler,
and only allows host-level configuration for scheduler parameters.
As a result, the \libos{} cannot emulate any Linux system APIs for configuring scheduler parameters,
such as \syscall{sched\_setparam}.
%Similar to memory management, the host ABI delegates scheduling to the hosts.
%As the design and implementation of the scheduling policies
%differ from host to host,
%the library OS cannot implement all the Linux scheduling API and policies in the user space,
%unless the host ABI exposes a wide interface
%for configuring the host scheduler.



Luckily, scheduler parameters does not impact most applications targeted by \graphene{}.
In general, applications can progress without setting scheduling priorities or policies, but may suffer poor performance.
A rare exception is when
an application set the CPU affinity of two collaborating threads to ensure concurrent execution.
%The only exception is CPU affinity, as binding a thread to one or multiple CPU cores.
Between a producer thread and a consumer thread,
failing to schedule the threads on individual CPU cores may cause the threads to deadlock.
Consider the following scenario:
the consumer thread A may busily wait for the producer thread B to deliver a new job, but never yield the CPU to allow thread B to proceed its execution.
%wherein the consumer busily waits for the producer.
%Such a producer-consumer model requires the producer and consumer threads to be scheduled on different CPU cores, to prevent being deadlocked by the scheduler.
% when an application requires
%producer and consumer threads
%to be scheduled on different cores,
%or the application may be deadlocked and stop progressing.
%which continues to poll a work queue;
%in this producer-consumer scenario,
%the threads must run on different CPU cores, to prevent deadlocks.
We propose adding a \hostapi{} called \palcall{ThreadSetCPUAffinity}
to support binding a thread to CPU cores:

\begin{paldef}
bool ThreadSetCPUAffinity (u8 cpu_indexes[], u8 num);
\end{paldef}

\fixmedp{Explain how the arguments work}
\palcall{ThreadSetCPUAffinity} binds the current thread to a list of CPU cores, as specified in \palkeyword{cpu_indexes}.
\palkeyword{cpu_indexes} is an array of non-negative integers, which must be smaller than the total number of CPU cores (specified in the PAL control block).



\papersubsubsection{Scheduling primitives}


\Thehostabi{} defines two scheduling primitives for synchronization between threads: mutually-exclusive (mutex) locking and event waiting.
These scheduling primitives improve user-space synchronization
implemented by atomic instructions or compare-and-swap (CAS).
The primitives prevent a thread from spinning on a CPU core
until the state of a lock or an event
is atomically changed,
%The host-level scheduling primitives
%%can prevent a thread from spinning on a CPU core,
%%by requesting the host scheduler
%can avoid the spinning
by suspending the thread in the host OS.





%, for synchronizing the execution
%of several threads running in parallel.
%The host ABI must provide scheduling primitives,
%because locking cannot be reliably implemented in the user space;
%user-space locking
%cannot prevent a thread from being interrupted inside a critical section,
%and thus 
%causing the application to deadlock.
%Also, user-space locking must be implemented using the compare-and-exchange (\code{CMPXCHG}) instructions, which may not be available on every architectures.
%Therefore, by including the scheduling primitives in the host ABI,
%\graphene{} can encapsulate different kinds of scheduling options available on the hosts.



\begin{paldef}
HANDLE MutexCreate (void);
void   MutexUnlock (HANDLE mutex_handle);
\end{paldef}


\palcall{MutexCreate} creates a handle for a mutex lock.
A mutex lock enforces atomic execution in a critical section:
if multiple threads are competing over a mutex lock
before entering the critical section,
only one thread can proceed while other threads will block until the lock is released again.
\palcall{MutexUnlock} releases a mutex lock held
by the current thread.
To acquire a mutex lock,
a generic \hostapi{},
\palcall{ObjectsWaitAny} (defined later),
can be used to compete with other threads,
or wait for the lock release if the lock is held.



\begin{paldef}
HANDLE SynchronizationEventCreate (void);
HANDLE NotificationEventCreate    (void);
void   EventSet   (HANDLE event_handle);
void   EventClear (HANDLE event_handle);
\end{paldef}



%\graphene{} inherit most of the scheduling primitives
%from \drawbridge{}~\cite{porter11drawbridge}:
%{\bf semaphores} ensures the atomicity of a critical section;
%{\bf events} enforces the order of execution among multiple threads.
%The events created by the host ABI
%can be separated into synchronization events and notification events;
%the former is used by a producer thread, to wake a consumer thread
%blocking on a queue;
%the latter notifies the occurrence of a condition
%and prevent further blocking of threads.
%Both semaphores and events resemblance the abstractions provided by the \win{} API;
%we show that these scheduling primitives are also portable on other hosts, including Linux, BSD, and \sgx{}, using futexes or similar locking mechanisms.



\palcall{SynchronizationEventCreate} and \palcall{NotificationEventCreate} create two different types of events.
Any thread can use \palcall{EventSet} to signal an event.
Signaling a synchronization event wakes up exactly one waiting thread to continue its execution.
A synchronization event coordinates threads that cooperate as producers and consumers;
a producer thread can signal exactly one blocking consumer at a time.
On the other hand, a notification event stays signaled until another thread
manually resets the event using \palcall{EventClear}.
A notification event object can be used for notifying the occurrence of a one-time event,
such as the start or termination of an execution.
%Both types of events can be waited by a thread. %, using \palcall{ObjectsWaitAny}.
%The difference is that
%singaling a synchronization event
%wakes up at most one thread that are currently waiting for the event,
%whereas signaling a notification event stops any thread from
%further blocking on the event.
\palcall{ObjectsWaitAny}
is also used
to wait for event signaling.


%Synchronization events and notification events are designed for different purposes.
%A synchronization event can be used in a producer-consumer model, wherein a consumer thread can block until a producer thread signals the event.
%On the other hand, a notification event can be used for notifying the occurrence of a one-time event;
%for example, a thread calling \palcall{ThreadCreate} can use a notification event to block until a new thread is created and starts execution.

The definition of the two scheduling primitives
covers two typical types of synchronization behaviors.
A mutex enforces atomicity of a critical
execution section.
An event enforces dependency relationship between
threads.
Both primitives are easy to port on most host OSes;
the host ABI directly adopts the definition from the \win{} API, and can be easily implemented
on Linux or similar OSes using futexes or POSIX thread (pthread) APIs. 


\papersubsubsection{Waiting for scheduling events}


%The host ABI allows the library OS to poll one or several handles
%at the same time.
%The possible handles to poll include semaphores, synchronization or notification events,
%or I/O streams.
%Therefore, the host ABI
%introduces a function, \palcall{ObjectsWait}, similar to \syscall{poll} in POSIX,
%with a timeout option.



\begin{paldef}
HANDLE ObjectsWaitAny (HANDLE *handle_array,
                       u8 handle_num, u64 timeout);
\end{paldef}


\palcall{ObjectsWaitAny} blocks the current thread for specific events listed in
a handle array (specified by \palkeyword{handle_array} and \palkeyword{handle_num}).
A common usage of \palcall{ObjectsWaitAny}
is to block on a scheduling primitive,
such as a mutex lock or a notification event.
If a certain event happens on one of the listed handles
\palcall{ObjectsWaitAny} resumes thread execution
and returns the handle to the caller. 
\palcall{ObjectsWaitAny} can only block on exactly one mutex lock or event, but can wait on
multiple I/O events.
When waiting on I/O events,
\palcall{ObjectsWaitAny} blocks until one of the listed stream handles receives incoming data or connections,
or encounters failures such as I/O stream shutdown.

%For the least,
%\palcall{ObjectsWaitAny} allows a thread to
%wait until a mutex lock is released, or an event is signaled.
%Moreover, \palcall{ObjectsWaitAny} can be used to poll %for certain events
%an I/O stream handle,
%to wait for events such as arrival or delivery of data,
%a stream being shut down.
%\palcall{ObjectsWaitAny}

\palcall{ObjectsWaitAny} takes a \palkeyword{timeout} argument to prevent waiting for an event indefinitely.
If the timeout expires before any event occurs,
%the occurrence of any events related with the given handles,
\palcall{ObjectsWaitAny} stops blocking
and returns no handle (NULL).


\palcall{ObjectsWaitAny} can also be used for polling multiple stream handles, to wait for I/O events
such as receiving inbound data or sudden failure.
Unlike a mutex lock or an event object, a stream handle can be associated
with multiple I/O events.
Therefore, the host ABI introduces a \hostapi{}, \palcall{StreamGetEvent}, to create a stream event handle
that represents a specific I/O event of the given stream handle.
The definition of \palcall{StreamGetEvent} is inspired by Bascule~\cite{baumann13bascule}.


\begin{paldef}
HANDLE StreamGetEvent (HANDLE stream_handle, u16 event);
\end{paldef}


\fixmedp{Could you accomplish the smae thing by creating more than one handle for a network socket?}
\palcall{StreamGetEvent} receives a stream handle and a specific I/O event.
The \palkeyword{event} argument can be given
one of the following values:
\palkeyword{READ_READY}, for notifying that there are inbound data ready to be read;
\palkeyword{WRITE_READY}, for notifying
that a network connection is fully established
and ready to be written;
and \palkeyword{ERROR}, for notifying that certain failures occur on the stream.

%A challenge to implementing polling
%to differentiate the I/O events which can occur on a stream.
%When blocking on a network or RPC stream, an application needs to be notified
%about three types of events:
%establishment of the connection (i.e., becoming writable), arrival of messages (i.e., becoming readable), and shut-down of the connection;
%thus, the \graphene{} host ABI introduces a new function,
%\palcall{StreamGetEvent},
%to generate a handle identifying these I/O events of a stream,
%and can be polled by \palcall{ObjectsWait}.
%The design also keeps the definition of \palcall{ObjectsWait} simple:
%\palcall{ObjectsWait} should do nothing more than blocking on an array of handles,
%until one of the handles
%is wakened, or the call timeouts.


\papersubsubsection{Thread-local storage}


On some OSes, such as Linux and \win{}, applications require a thread-local storage (TLS)
to store thread-private variables or maintain a thread control block (TCB).
On \graphenearch{},
TLS is often referenced by one of the FS and GS segment registers,
to improve the performance of accessing any variable in the TLS.
%from all other threads.
%Retrieving or assigning the value of the FS/GS register
%is a privilege operation
%and must be performed in the host kernel.
As a convention of Linux, many Linux application executables
contain hard-coded access to TLS
using the FS register.
Since setting the value of the FS/GS registers is a privileged operations,
\thehostabi{} requires a call to enter the host kernel
and set the registers for referencing TLS.


%Because thread-local storage can be frequently accessed
%in an application,
%it would be inefficient to constantly
%enter the kernel for retrieving the TLS data.
%A common design is to occupy one of the thread-private registers
%to store a pointer to the thread-local storage;
%on \graphenearch{}, the FS register is commonly used to reference the thread-local storage.




\begin{paldef}
u64 SegmentRegisterAccess (u8 register, u64 value);
\end{paldef}




\Thehostabi{} introduces a \hostapi{}, \palcall{SegmentRegisterAccess}, for reading or writing the FS/GS register value.
The \palkeyword{register} argument can be either \palkeyword{WRITE_FS} or \palkeyword{WRITE_GS},
with the \palkeyword{value} argument
being a pointer that references to the TLS area.
Otherwise, the \palkeyword{register} argument
can be \palkeyword{READ_FS} or \palkeyword{READ_GS},
to retrieve the FS/GS register value.
On success, the \hostapi{} returns the current value of FS/GS register.



\issuedone{1.3.e}{Discuss the FS/GS limitation}
Unfortunately, the feasibility of implementing \palcall{SegmentRegisterAccess}
depends on the host OS.
%The implementation of \palcall{SegmentRegSet}
%depends the host system interfaces. 
%By default, reading or writing the value of the FS/GS registers is a privileged operation
%which can only be performed in ring 0.
Linux and similar OSes allow the usage of FS/GS register,
primarily because the FS register is heavily used in the standard C library.
%or BSD, the FS register is commonly used for bookkeeping in the standard C library;
%thus, the Linux or BSD system calls naturally include the feature of reading or writing the FS/GS registers in the kernel space (although only the FS register is used in the user space).
However, in other OSes, especially \win{} and \osx{},
changing the FS/GS register %is considered an unnecessary or even dangerous behavior,
is forbidden by the OS kernels.
The \win{} 7, 8, and 10 kernels confiscate the FS register for storing a thread control block (TCB),
and thus forbid users to change the FS register value.
\osx{}'s xnu kernel considers FS/GS registers to be of no concrete use.
These OS kernels
periodically reset the FS/GS registers
to mitigate any user attempt
of changing them.
If a host OS fails to implement \palcall{SegmentRegisterAccess},
the \libos{} may have to develop workarounds such as binary translation
to virtualize TLS access.


%we observe the case where changing the FS/GS registers is considered unnecessary and dangerous to the kernel.
%These hosts periodically reset the value of the FS/GS registers,
%preventing the host ABI to assign the TLS permanently.



%The primary challenge
%to implementing TLS for Linux applications
%is that an executable can hard-code the references to the FS register in its binary.
%Because a Linux executable is usually {\em unrelocatable},
%it can access a thread-private pointer by simply reading or writing to a specific offset
%to the FS register.
%To support these applications,
%a host must populate a valid TCB at the address pointed by the FS register;
%if the host forbids setting the FS register,
%the library OS cannot support TLS for an executable using thread-private pointers.

%On these hosts, the compatibility for TLS usage in applications is partially sacrificed.



\papersubsection{Processes}
\label{sec:abi:proc}


\fixmedp{don't mix perspective; focus in this chapter on the PAL model only. I would do minimal discussion of what the libOS does here.}
\Thehostabi{} creates clean, brand-new processes
for multi-process applications.
A process, or a {\bf picoprocess} in the perspective of the \libos{}, contains a new PAL instance,
a new \libos{},
and the application specified to \thehostabi{}.
\Thehostabi{} is designed to simplify porting a multi-process applications,
by dropping the assumption of
coherent memory sharing across processes.
Therefore, \thehostabi{} chooses a completely different process creation model
from the typical copy-on-write forking model
of UNIX-style OSes.

%The host ABI defines a process
%as a simple abstraction, which owns a new address space, and starts with a clean state of no guest VMAs, no held I/O resources, and no allocated handles.
%Moreover, a new process will not share any memory with former processes.
%The definition of the process abstraction
%is meant to simplify the host design for expanding a single-process execution to multiple processes.




%\graphene{} implements a distributed OS model for multi-process applications.
%Each process in \graphene{} has a library OS instance;
%Multiple library OS instances in a multi-process application must work together
%to present a single OS view,
%same as running in a native Linux.
%Therefore, the host ABI does not require copy-on-writing forking,
%but simply creation of a clean process instantiating the library OS.




\begin{paldef}
HANDLE ProcessCreate (const char *application_uri,
                      const char *manifest_uri,
                      const char **args, uint flags);
\end{paldef}


\palcall{ProcessCreate} creates a clean process to load an application executable specified by
\palkeyword{application_uri}. %, a URI that identifies the application executable.
\palcall{ProcessCreate} also allows specifying a manifest file (\palkeyword{manifest_uri}) for user policy configuration, as well as command-line arguments (\palkeyword{args}) passed to the new process.
\palcall{ProcessCreate} is equivalent to
relaunching the specified application in \graphene{},
except two distinctions: (1) \palcall{ProcessCreate} returns a process handle to its caller;
(2) a process created by \palcall{ProcessCreate} naturally belongs to the same {\em sandbox} as its parent.
%---an isolated container of related processes---with its parent process.
The detail of the sandbox abstraction is discussed in Section~\ref{sec:abi:proc:sandbox}.




%When creating a process, the host ABI, as \palcall{ProcessCreate}, takes a URI of the executable to run in the new process,
%together with a manifest file, which specifies the user policy.
%Once a new process is created, the library OS can be initialized
%and migrate the library OS and application state from the parent process;
%after migration, the new process can resume execution from the point of checkpointing,
%as a forked process.
%To the hosts, the processes in an application share nothing
%but the I/O streams opened by multiple processes.


%\palcall{ProcessCreate} returns a process handle.
%To bootstrap the inter-process communication, a process handle also works as an unnamed RPC stream connecting the parent and child processes.
%The guests in the parent and child processes can use this RPC stream to share internal states,
%as well as to inherit I/O stream handles from each other.

%The initialization of a library OS instance uses the RPC stream
%to retrieve namespace coordination information, such as how to locate the namespace leaders.
%The PRC stream is also used to send process migration data
%from the parent process, to implement forking.






\papersubsubsection{Sharing a handle}




Due to the statelessness of handles,
a guest can cleanly migrate its state to a new process, and recreate all handles afterward.
Unfortunately, not all I/O streams can be recreated
in a new process, due to the host limitations;
for instance, most host OSes bound network connections with the processes that first accept the connections,
and only allow sharing connections through inheriting
file descriptors from the parent process.
%is inherited through cloning.
%only identified by one handle or file descriptor,
%and can only be shared when the handle or file descriptor
%is inherited by a child process.
%The same inheritance feature also has to be implemented
%inside the library OS, to support Linux applications.
Since every process created by \palcall{ProcessCreate} is a clean picoprocess without inheriting any stream handles,
a guest needs a host feature
to share a network stream handle with other processes.


\begin{paldef}
void   RpcSendHandle (HANDLE rpc_handle, HANDLE cargo);
HANDLE RpcRecvHandle (HANDLE rpc_handle);
\end{paldef}



\Thehostabi{} introduces \palcall{RpcSendHandle} and \palcall{RpcRecvHandle} for sharing I/O stream handles over a RPC stream (a process handle is also used as a RPC stream).
\palcall{RpcSendHandle} 
migrates the host state of a stream handle, specified by \palkeyword{cargo},
over a RPC stream handle.
\palcall{RpcSendHandle},
which is called inside another process,
then receives the migrated host states from the RPC stream.
\palcall{RpcSendHandle}
will grant the receiving process permissions to access the I/O stream handle. % with the sending process.
If \palcall{RpcSendHandle} succeeds, it returns a handle
that references to the shared I/O stream.
The abstraction is similar to a feature in Linux and similar OSes that
shares file descriptors over a UNIX domain socket.

\papersubsubsection{Bulk IPC (physical memory store)}


%Migrating the guest state over a RPC stream can suffer significant overhead,
%when copying large chunks of memory.
%Especially, the RPC-based migration will slow down the
%latency of copy-on-write forking.
%the key overhead
%is caused by copying large chunk of memory
%across processes, without the help of a host to share the physical memory.



\Thehostabi{} introduces an optional bulk IPC feature, as a alternative to RPC streams.
The optimization brought
by the feature
is to reduce the latency of sending large chunks of data across processes.
The main abstraction of bulk IPC is a physical memory store.
Multiple processes can open the same memory store;
a processes sends the data in a piece of page-aligned memory to the store,
while another process maps the data to its memory.
Since the host can enable the copy-on-write sharing on the data mapped to both processes,
the latency can be much shorter than copying the data over a RPC stream.


%can send application memory to the physical memory store (using \palcall{PhysicalMemoryCommit}), which will keep a snapshot of the memory, set as copy-on-write.
%The child process can then attach to the physical memory store,
%and map the memory snapshot into its memory (using \palcall{PhysicalMemoryMap}).
%The Bulk IPC feature can large reduce the amount of physical memory copied during process migration,
%and thus optimize the latency of forking.


\begin{paldef}
HANDLE PhysicalMemoryStore  (u32 index);
\end{paldef}


\palcall{PhysicalMemoryStore} creates or attaches to a physical memory store,
based on a given index number.
The indexing of physical memory stores is independent for each sandbox (the container abstraction discussed in Section~\ref{sec:abi:proc:sandbox}),
so unrelated guests cannot share a physical memory store by specifying the same index number.
If \palcall{PhysicalMemoryStore} succeeds,
it returns a handle that references to the physical memory store.
The store is alive until every related processes close the corresponding store handles,
and no data is left in the store.




\begin{paldef}
u64 PhysicalMemoryCommit (HANDLE store_handle, u64 addr, u64 size);
u64 PhysicalMemoryMap    (HANDLE store_handle, u64 addr, u64 size,
                          u16 protect_flags);
\end{paldef}


\palcall{PhysicalMemoryCommit} commits the data in a memory range to a physical memory store.
Both \palkeyword{addr} and \palkeyword{size} must be aligned to pages,
so that the host can enable copy-on-write sharing if possible.
\palcall{PhysicalMemoryMap} maps the data from a physical memory store
to a memory range in the current process.
\palkeyword{protect_flags} specifies the page protection assigned to the mapped memory ranges.




\papersubsection{Sandboxing}
\label{sec:abi:proc:sandbox}


The security isolation of \graphene{} is based on a {\bf sandbox}, a container isolating a number of coordinating library OS instances.
When \graphene{} launches an application, the application begins running inside a standalone sandbox.
By default, a new process cloned by the application share the sandbox
with its parent process.
To configure the isolation policies,
developers provide a {\bf manifest} file for each application.
The policies are enforced by a reference monitor in the host.
A manifest file contains run-time rules for sandboxing resources which can be shared in the host,
including files, network sockets, and RPC streams.



Sandboxes delegate
enforcement of security isolation to the host OSes.
An application doesn't have to trust the library OS
to enforce security policies,
on every applications running on the same host.
If a library OS instance is compromised by the application,
the threat will be contained inside the sandbox,
and cannot cross the sandbox boundary, unless the host is also compromised.
For each sandbox,
the isolation policies are statically assigned,
in the manifest file given at the launch.
The isolation policies
cannot be subverted during execution.



\Thehostabi{} also introduces a \hostapi{}, \palcall{SandboxSetPolicy},
to dynamically move a process to a new sandbox.
Sometimes, an application needs to reassign the rules of security isolation,
for enforcing stricter rules inside the application.
A multi-sandbox environment can protect an application with multiple privilege levels, or an application that creates session for separating the processing for each client.
%belong to different sessions which should be isolated from each other.
With \palcall{SandboxSetPolicy}, a process that requires less security privilege
or serves a separate session can voluntarily moves itself to a new sandbox,
with stricter rules.
\palcall{SandboxSetPolicy} can dynamically
assign a new manifest file that specifies the new rules,
to be applied to the
new sandbox created for the current process.
%to restrict a process from accessing the resources shared by other processes.
%\palcall{SandboxSetPolicy}
%moves a process to a new sandbox.
%An option, \palkeyword{sandbox_rpc}, can block all RPC streams from other processes running in the original sandbox.







\begin{paldef}
bool SandboxSetPolicy (const char *manifest_uri,
                       u16 sandbox_flags);
\end{paldef}


\palcall{SandboxSetPolicy} receives a URI of the manifest file that specifies the sandboxing rules,
and an optional \palkeyword{sandbox_flags} argument.
The \palkeyword{sandbox_flags} argument currently can only contain one value:
\palkeyword{SANDBOX_RPC}, for isolating the RPC streams between the original sandbox and the new sandbox.
