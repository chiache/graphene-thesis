\subsection{Threading}
\label{sec:abi:thread}

\issuedone{1.2.a}{discuss resource management at host level (threading)}
Threading in the host ABI includes two kinds of features:
one is to create a thread, for scheduling the CPU resources; the other is to synchronize or coordinate among multiple threads,
using several scheduling primitives programmable by applications.
Both Linux and POSIX have defined a rich threading API, with various thread creation options
and scheduling primitives.
\graphene{} simplifies the host ABI
to include only a small set of threading features.
%for each host to implement.



\subsubsection*{Creating a thread}



Unlike a user thread in Linux or POSIX (i.e., a ``pthread''),
a guest thread created by the host ABI,
using \palcall{ThreadCreate},
is simply a new context which starts at a function.
The host ABI
moves all the thread creation options,
such as thread-local storage,
to an initialization function called after the thread creation,
inside the library OS.
The purpose of \palcall{ThreadCreate}
is simple: it creates a ``kernel thread'' in the host,
which can be scheduled by the host scheduler,
to run application code on another CPU core.
The \graphene{} library OS does not implement its own scheduler.


\begin{paldef}
HANDLE ThreadCreate (void (*start) (void *), void *arg);
void ThreadExit (void);
\end{paldef}


The host ABI does not implement thread exiting notifications.
When a guest thread is terminated using \palcall{ThreadExit}, the PAL destroys the correspondent kernel thread
without notifying other threads.
The design counts on the library OS to implement the Linux-style notifications,
including
sending a \code{SIGCHLD} signal,
or triggering a parent-wakening futex.
% assigned by a child thread ID field given to  




\subsubsection*{Scheduler}




Besides thread creation,
the host ABI needs scheduling features to interrupt a thread execution,
or yield the CPU.
The concerns for including scheduling features
in the host ABI
is two-fold.
The first concern is regarding compatibility;
an application may be stuck in a busy-waiting loop, if the library OS lacks the ability to interrupt the execution when certain events occur.
Specifically, \palcall{ThreadInterrupt} interrupts a thread using a thread handle
returned by \palcall{ThreadCreat}.
The second concern is regarding the CPU occupancy;
if a thread does not forfeit the CPU when it stops making progress,
the CPU can be wasted being idle.
\palcall{ThreadDelay} and \palcall{ThreadYield}
delays the current thread, until a period of time has passed, or the host scheduler re-schedules the thread to a CPU core.



\begin{paldef}
bool ThreadDelay     (ulong delay_microsec);
void ThreadYield     (void);
void ThreadInterrupt (HANDLE thread_handle);
\end{paldef}



Similar to memory management, the host ABI delegates scheduling to the hosts.
As the design and implementation of the scheduling policies
differ from host to host,
the library OS cannot implement all the Linux scheduling API and policies in the user space,
unless the host ABI exposes a wide interface
for configuring the host scheduler.
Luckily, most of the scheduling options in Linux does not impact
the functionality of an application;
for example, without configuring the scheduling priority of threads,
the application can still make progress,
but may suffer performance penalty due to unnecessary CPU idles.
The only exception happens when an application requires
producer and consumer threads
to be scheduled on different cores,
or the application may be deadlocked and stop progressing.
%which continues to poll a work queue;
%in this producer-consumer scenario,
%the threads must run on different CPU cores, to prevent deadlocks.
We propose adding a function called \palcall{ThreadSetCPUInfinity} to the host ABI,
to support binding a thread to CPU cores:

\begin{paldef}
bool ThreadSetCPUInfinity (uint cpu_nums[], uint count);
\end{paldef}


\subsubsection*{Scheduling primitives}


The host ABI also includes scheduling primitives, such as locking, for synchronizing the execution
of several threads running in parallel.
The host ABI must provide scheduling primitives,
because locking cannot be reliably implemented in the user space;
user-space locking
cannot prevent a thread from being interrupted inside a critical section,
and thus 
causing the application to deadlock.
Also, user-space locking must be implemented using the compare-and-exchange (\code{CMPXCHG}) instructions, which may not be available on every architectures.
Therefore, by including the scheduling primitives in the host ABI,
\graphene{} can encapsulate different kinds of scheduling options available on the hosts.



\begin{paldef}
HANDLE SemaphoreCreate  (uint max_count, uint inital);
void   SemaphoreRelease (HANDLE semaphore_handle);
void   SemaphoreDestroy (HANDLE semaphore_handle);
HANDLE SynchronizationEventCreate (void);
HANDLE NotificationEventCreate    (void);
void EventSet     (HANDLE event_handle);
void EventDestroy (HANDLE event_handle);
\end{paldef}



\graphene{} inherit most of the scheduling primitives
from \drawbridge{}~\cite{porter11drawbridge}:
{\bf semaphores} ensures the atomicity of a critical section;
{\bf events} enforces the order of execution among multiple threads.
The events created by the host ABI
can be separated into synchronization events and notification events;
the former is used by a producer thread, to wake a consumer thread
blocking on a queue;
the latter notifies the occurrence of a condition
and prevent further blocking of threads.
Both semaphores and events resemblance the abstractions provided by the Windows API;
we show that these scheduling primitives are also portable on other hosts, including Linux, BSD, and SGX, using futexes or similar locking mechanisms.









\subsubsection*{Polling handles}


The host ABI allows the library OS to poll one or several handles
at the same time.
The possible handles to poll include semaphores, synchronization or notification events,
or I/O streams.
Therefore, the host ABI
introduces a function, \palcall{ObjectsWait}, similar to \syscall{poll} in POSIX,
with a timeout option.



\begin{paldef}
HANDLE ObjectsWait    (HANDLE *handles, uint nhandles,
                       ulong timeout);
HANDLE StreamGetEvent (HANDLE stream_handle,
                       uint event_flags);
\end{paldef}


A challenge to implementing polling
to differentiate the I/O events which can occur on a stream.
When blocking on a network or RPC stream, an application needs to be notified
about three types of events:
establishment of the connection (i.e., becoming writable), arrival of messages (i.e., becoming readable), and shut-down of the connection;
thus, the \graphene{} host ABI introduces a new function,
\palcall{StreamGetEvent},
to generate a handle identifying these I/O events of a stream,
and can be polled by \palcall{ObjectsWait}.
The design also keeps the definition of \palcall{ObjectsWait} simple:
\palcall{ObjectsWait} should do nothing more than blocking on an array of handles,
until one of the handles
is wakened, or the call timeouts.


\subsubsection*{Thread-local storage}


Most OSes requires thread-local storage (TLS),
either to implement thread-private application variables, or to store thread-specific, system library states, in a thread control block (TCB).
Because thread-local storage can be frequently accessed
in an application,
it would be inefficient to constantly
enter the kernel for retrieving the TLS data.
A common design is to occupy one of the thread-private registers
to store a pointer to the thread-local storage;
on \graphenearch{}, the FS register is commonly used to reference the thread-local storage.




\begin{paldef}
void SegmentRegSet (uint register, ulong *value);
\end{paldef}



\issuedone{1.3.e}{discuss the FS/GS limitation}
The host ABI includes a function, \palcall{SegmentRegSet}, to set the FS/GS registers.
The implementation of \palcall{SegmentRegSet}
depends the host system interfaces. 
By default, reading or writing the value of the FS/GS registers is a privileged operation
which can only be performed in ring 0.
On Linux or BSD, the FS register is commonly used for bookkeeping in the standard C library;
thus, the Linux or BSD system calls naturally include the feature of reading or writing the FS/GS registers in the kernel space (although only the FS register is used in the user space).
However, on other hosts, especially Windows and OSX,
we observe the case where changing the FS/GS registers is considered unnecessary and dangerous to the kernel.
These hosts periodically reset the value of the FS/GS registers,
preventing the host ABI to assign the TLS permanently.


The primary challenge
to implementing TLS for Linux applications
is that an executable can hard-code the references to the FS register in its binary.
Because a Linux executable is usually {\em unrelocatable},
it can access a thread-private pointer by simply reading or writing to a specific offset
to the FS register.
To support these applications,
a host must populate a valid TCB at the address pointed by the FS register;
if the host forbids setting the FS register,
the library OS cannot support TLS for an executable using thread-private pointers.

%On these hosts, the compatibility for TLS usage in applications is partially sacrificed.



\subsection{Processes}
\label{sec:abi:proc}


\graphene{} implements a distributed OS model for multi-process applications.
Each process in \graphene{} has a library OS instance;
Multiple library OS instances in a multi-process application must work together
to present a single OS view,
same as running in a native Linux.
Therefore, the host ABI does not require copy-on-writing forking,
but simply creation of a clean process instantiating the library OS.




\begin{paldef}
HANDLE ProcessCreate (const char *executable,
                      const char *manifest,
                      const char **args, uint flags);
\end{paldef}

When creating a process, the host ABI, as \palcall{ProcessCreate}, takes a URI of the executable to run in the new process,
together with a manifest file, which specifies the user policy.
Once a new process is created, the library OS can be initialized
and migrate the library OS and application state from the parent process;
after migration, the new process can resume execution from the point of checkpointing,
as a forked process.
To the hosts, the processes in an application share nothing
but the I/O streams opened by multiple processes.


\palcall{ProcessCreate} returns a process handle.
To bootstrap the inter-process communication, a process handle also works as an unnamed RPC stream connecting the parent and child processes.
The initialization of a library OS instance uses the RPC stream
to retrieve namespace coordination information, such as how to locate the namespace leaders.
The PRC stream is also used to send process migration data
from the parent process, to implement forking.






\subsubsection*{Sharing a handle}




Due to the statelessness of stream handles,
a library OS can cleanly migrate its state to a new process, and recreate all the stream handles previously opened in the process.
Unfortunately, not all I/O streams can be recreated
in a new process, due to the limitations of host system interfaces;
in most hosts, a network connection is only identified by one handle or file descriptor,
and can only be shared when the handle or file descriptor
is inherited by a child process.
The same inheritance feature also has to be implemented
inside the library OS, to support Linux applications.
Since every process created by \palcall{ProcessCreate} is a clean \graphene{} process with freshly-initialized library OS state,
the library OS needs extra functions in the host ABI
to inherit handles instead of data from the parent process.


\begin{paldef}
void   RpcSendHandle (HANDLE rpc_handle, HANDLE cargo);
HANDLE RpcRecvHandle (HANDLE rpc_handle);
\end{paldef}



The host ABI introduces two functions for sharing handles across processes: \palcall{RpcSendHandle} and \palcall{RpcRecvHandle}, which sends handles over a connected RPC stream.
The design is motivated by the Linux file descriptor sharing feature of UNIX domain sockets.
\palcall{RpcSendHandle} will migrate the state of a stream handle, created by \palcall{StreamOpen},
over a RPC stream, to the receiving process, which calls \palcall{RpcRecvHandle}.
\palcall{RpcSendHandle} will grant the receiving process the permission, to share the stream handle with the sending process.


\subsubsection*{Bulk IPC (physical memory store)}


Migrating and coordinating process states over RPC streams
can suffer significant overhead, when copying large chunks of memory.
Especially, the RPC-based migration will slow down the latency of copy-on-write forking.
%the key overhead
%is caused by copying large chunk of memory
%across processes, without the help of a host to share the physical memory.

\begin{paldef}
HANDLE PhysicalMemoryStore  (uint index);
ulong  PhysicalMemoryCommit (HANDLE store_handle,
                             void *addr, ulong size);
void * PhysicalMemoryMap    (HANDLE store_handle,
                             void *addr, ulong size,
                             uint protection);
\end{paldef}


The host ABI introduces a Bulk IPC feature, for reducing the latency of forking.
The main abstraction of Bulk IPC
is a physical memory store (created by \palcall{PhysicalMemoryStore});
To migrate a process, the library OS
can send application memory to the physical memory store (using \palcall{PhysicalMemoryCommit}), which will keep a snapshot of the memory, set as copy-on-write.
The child process can then attach to the physical memory store,
and map the memory snapshot into its memory (using \palcall{PhysicalMemoryMap}).
The Bulk IPC feature can large reduce the amount of physical memory copied during process migration,
and thus optimize the latency of forking.




\subsubsection*{Sandboxing}


The security isolation of \graphene{} is based on a {\bf sandbox}, a container isolating a number of coordinating library OS instances.
When \graphene{} launches an application, the application begins running inside a standalone sandbox.
By default, a new process cloned by the application share the sandbox
with its parent process.
To configure the isolation policies,
developers provide a {\bf manifest} file for each application.
The policies are enforced by a reference monitor in the host.
A manifest file contains run-time rules for sandboxing resources which can be shared in the host,
including files, network sockets, and RPC streams.



Sandboxing delegates
security isolation to the host.
An application doesn't have to trust the library OS
to enforce security policies,
on every applications running on the same host.
If a library OS instance is compromised by the application,
the threat will be contained inside the sandbox,
and cannot cross the sandbox boundary, unless the host is also compromised.
For each sandbox,
the isolation policies are statically assigned,
in the manifest file given at the launch.
The isolation policies
cannot be subverted during execution.



However, sometimes, an application may want to reassign the policies,
for security isolation inside of the application.
Processes in an application may have different privilege levels, or belong to different sessions which should be isolated from each other.
\palcall{SandboxSetPolicy} can dynamically
assign a new manifest file specifying a stricter policy
than the original manifest,
to restrict a process from accessing the resources shared by other processes.
\palcall{SandboxSetPolicy}
moves a process to a new sandbox.
An option, \palkeyword{sandbox_rpc}, can block all RPC streams from other processes running in the original sandbox.







\begin{paldef}
bool SandboxSetPolicy (const char *sandbox_manifest,
                       bool sandbox_rpc);
\end{paldef}



