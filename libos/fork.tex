%\section{User-level {\tt fork} and other Linux library OS challenges}
%%% \vspace{5pt}
%%% \label{sec:fork}
%%% \noindent {\bf Copy-On-Write Fork.~}
%%% Creating a clean guest eases reasoning about security isolation, as all shared abstractions
%%% must be implemented using explicit data streams  between guests.
%%% This section describes how we implement these Unix abstractions in the guest
%%% %achieve a sensible division of labor between guest and host, and 
%%% without baking Unix personality into the host ABI.


\begin{comment}
\vspace{5pt}
\noindent{\bf Guest self-migration.~}  One of the key features of the host ABI
is that guest state can be programmatically read and recreated.
As a result, guests can checkpoint, migrate, and resume themselves in a new picoprocess,
potentially on a new host.  
Most of the library OS and application state are checkpointed simply 
by copying the contents of virtual memory into a file.
Checkpointing requires manually serializing a few key data structures
in {\tt libLinux} that are needed to resume the library OS from a checkpoint,
% interface directly with the PAL, 
including the thread states, handle table, and memory mappings.  

Resuming from a checkpoint involves restoring these key data
structures (handles, thread register contexts, memory mappings), and re-loading memory
contents from the checkpoint.  Most additional data structures
in {\tt libLinux}, and all application data structures,
are reloaded at the virtual address as before the checkpoint and work without modification.
\end{comment}

\begin{comment}
When a new guest begins execution, an input argument to {\tt libLinux} indicates
whether control should be transferred to the Linux loader ({\tt ld.so}) to start a new application instance, 
or whether a checkpoint should be loaded instead.
\end{comment}
%After the checkpoint is loaded, all threads resume execution on their stacks.


\section{Multi-Process Abstractions} 

%A characteristic of Linux application development is the opportunity of using one of 
This section explains the \thelibos{} implementation of
the plenty Linux multi-process abstractions,
including forking, \syscall{execve}, and a variety of IPC (inter-process communication) mechanisms. 




\subsection{Forking a process}



Implementing the UNIX-style, copy-on-write forking presents a particular challenge to the \graphene{} architecture.
%when implemented using only a VM-like picoprocess abstraction.
As with a virtual machine, a \graphene{} host creates each new \picoproc{} in a ``clean'' state, along with an individual \thelibos{} instance. % fork is implemented in the \libos{}.
%yet applications require common Unix abstractions such as file handle inheritance and copy-on-write fork.
Forking a process includes cloning the state of running application and migrating all the resource handles inside \thelibos{}, such as file descriptors, to the new \picoproc{}.
\graphene{} revokes the assumption that each of its hosts is capable of sharing physical pages between multiple applications or processes.
Since \thelibos{} cannot enable copy-on-write sharing between \picoprocs{}, \thelibos{} needs an elaborate but efficient scheme for emulating the UNIX-style forking.


\issuedone{1.3.b}{describe the work flow of forking}
Without host support of copy-on-write sharing,
\thelibos{} emulates \syscall{fork} by checkpointing and migrating the process states.
When an application forks a process,
the current \thelibos{} instance holds a list of process resources that must be cloned for the new process.
By checkpointing the process states,
\thelibos{} creates a snapshot of the current process,
which is expected to the initial state
of the new process,
%expects to be reproduced to new process,
except a few minor changes.
A process snapshot includes all allocated resources, such as VMA and file handles,
and miscellaneous process states, such as signal handlers.
After checkpointing,
\thelibos{} can send the process snapshot over a RPC stream, so that the state of the forking process is faithfully migrated to a new \thelibos{} instance.


To emulate \syscall{fork}, \thelibos{} implements
a checkpoint and migration scheme
for duplicating the resource handles and application states between \picoprocs{}.
For each type of resources, \thelibos{} defines a function for decomposing a resource handle in a migratable form,
and a function for reconstructing the resource handle inside another \picoproc{}.
For example,
for a VMA handle, \thelibos{} checkpoints the address, size, initial flags, and page protection,
and only if the VMA is accessed by the application and not backed by a file,
\thelibos{} copies the memory data into the snapshot.
For a file or network handle,
\thelibos{} runs a virtual file system operation,
\funcname{vfs\_checkout},
to externalize the related states
inside the file system implementations,
but skip any temporary states such as buffers and directory cache entries.
Finally, \thelibos{} checkpoints the current thread handle, but modifies the copy with a new process ID.



The checkpoint and migration scheme of \thelibos{} is comparable to
VM migration by a hypervisor.
When migrating a VM, a hypervisor has to copy the VM's guest physical memory to another host machine.
A useful feature of a hypervisor is to migrate a live VM,
and to implement the feature, the hypervisor needs hardware support for marking the dirty pages when it is copying the pages.
\graphene{} also implements live migration of a \picoproc{} for \syscall{fork}
because of the general expectation
that \syscall{fork} should not halt the whole process.
However, unlike live VM migration, \graphene{} chooses not %adopt the technique of live VM migration
copy the whole virtual address space
of a \picoproc{}
%Instead, the checkpoint scheme of \thelibos{}
%locks each handle when copying it to the heap or over a RPC stream
%\graphene{} chooses not to copy the whole virtual address space 
for three primary reasons.
First, a checkpoint scheme that snapshots the whole \picoproc{}
cannot differentiate temporary and permanent states inside a \libos{}.
To improve I/O performance, \thelibos{} tends to reserve %a significant amount of
virtual memory for caching and buffering, and \thelibos{} can reduce migration time by skipping temporary states such as directory cache entries and I/O buffers.
Second, by checkpointing handles individually,
\thelibos{} can overwrite each handle and sanitize any sensitive states before sending the snapshot out to another \picoproc{}.
Finally, \thehostabi{} does not export any functionality for tracing
the dirty pages during live migration,
because the low-level hardware support needed
is not available on a more restricted hardware like Intel SGX.
For all the reasons above,
\thelibos{} only selectively checkpoints \libos{} states rather than snapshotting the whole \picoproc{}.


%new \picoproc{} cannot reuse PAL handles snapshotted from the previous \picoproc{}.
%Therefore, after migrating the whole virtual address space,
%either \thelibos{} has to scan its internal heap to recreate or nullify all the PAL handles, or \thehostabi{} has to detect any migrated PAL handles.
%Moreover, to consistently snapshot a \picoproc{},
%\thelibos{} has to stop the execution of every running threads,
%or iteratively trace dirty pages after copying a certain amount of pages to the snapshot.
%Either approach requires more control over host-level page management
%in order to prevent significant interruption in the application.






%Rather than writing the checkpoint to a file, 
%we developed an efficient bulk IPC mechanism to 
%permit copy-on-write sharing of memory pages among processes.
%Bulk IPC is a performance optimization over sending each byte of the parent address
%space over a stream, although {\tt libLinux} can also implement {\tt fork}
%over a stream.
%Bulk IPC adds 3 calls to the host ABI,
%and the host reference monitor only permits bulk IPC among
%picoprocesses within a sandbox.

%% Conceptually, {\tt fork} could be implemented by checkpointing the parent,
%% modifying the primary thread's checkpoint 
%% so that the child returns 0 from the fork call 
%% (indicating it is the child),
%% and then immediately resuming the checkpoint in another picoprocess.

%% In practice, we optimize {\tt fork} performance by avoiding 
%% the use of an intermediate checkpoint file, instead transferring the checkpoint
%% directly to the child over a host-level bulk IPC
%%  mechanism (\S\ref{sec:linux:pal}).
%The Graphene bulk IPC abstraction adds 3 PAL calls 
%that allow guests to efficiently transfer large regions of memory to each other\fixmedp{after reordering, add a forward or back ref}.



Migrating process states over a RPC stream adds a significant overhead to the latency of \syscall{fork} in \thelibos{}.
To reduce the overhead, \graphene{} introduces a {\bf bulk IPC} mechanism in \thehostabi{}, to send large chunks of memory across \picoprocs{}. 
Using the bulk IPC mechanism, 
%Using our bulk IPC mechanism,
the sender (i.e., the parent) can request the host kernel
to preserve the physical pages of application memory and snapshot data,
and the receiver (i.e., the child) can map these physical pages to its own virtual address space.
This bulk IPC mechanism is an efficiency way of sending pages out-of-band,
while the parent process still uses a RPC stream
to send control messages including the parameters of bulk IPC.
Although the implementation is up to the host kernel,
the bulk IPC mechanism should map the same physical pages in both parent and child,
to minimize the memory copy in the host kernel.
The host kernel marks the physical pages copy-on-write in both \picoprocs{},
to ensure that the child receives a snapshot of the sent pages from the parent without sharing any future changes.




%Our IPC module is \gipclines{} lines of code (Table~\ref{tab:graphene:loc}), 
%runs on multiple versions of Linux (2.6 and 3 series kernels), and
%does not require
%Linux kernel changes or recompilation.


\begin{comment}
A critical challenge in developing a Linux library OS was implementing 
handle inheritance in the guest.  In some cases, 
handles are easy to reproduce: an open file can simply be reopened in the child,
and the cursor offset adjusted (note that file handle offsets are a library abstraction
implemented over a memory mapped file).
Pipes, however, are not easily recreated without host support.
\end{comment}
%One option was to create explicitly named host-level byte streams,
%similar to System V or Windows named pipes.
%This strategy is simple to add to the Drawbridge host ABI and easy to program in {\tt libLinux},
%but complicates security isolation, as guests must be prevented from 
%opening a host-level pipe outside of their sandbox.

%A second option, which we pursue, is to only create anonymous bytes streams,
\paragraph{Inheriting File Handles.}
\thelibos{} uses two \hostapis{}, \palcall{RPCSendHandle} and \palcall{RPCRecvHandle}, to transfer 
stream handles out-of-band over reviously 
established byte streams within a sandbox.  Handle passing facilitates inheritance
and general-purpose RPC.
This mechanism is similar to Unix Domain Sockets,
which are commonly used by sandboxing systems. % such as plash~\citep{plash}.
This strategy allows a guest to seamlessly and explicitly 
share an open handle with another guest in the same sandbox, but prevents
a guest from sharing a handle with a guest outside of the sandbox.

\begin{comment}
\vspace{5pt}
\noindent{\bf Discussion.~}
A Graphene picoprocess can copy part or all its address space into a child
picoprocess relatively efficiently.
Although this mechanism is less efficient than an in-kernel {\tt fork},
we wanted to maintain the generality benefits of recent \liboses{},
and only added the minimal building blocks to the host ABI.
%we felt this design would bake Unix personality into the host kernel ABI,
%and  reintroduce security problems caused by accidental inheritance~\citep{close-on-exec}.
The transfer of data is explicit to the host, can be mediated by a reference monitor,
the sender, or the receiver.
For instance, recent Unix systems introduced a close-on-exec flag for file handles~\citep{close-on-exec}, 
which prevents inheritance of handles to sensitive files.  This can be implemented
either in a parent, by excluding the file handle from a checkpoint, 
or in the child, by closing this handle on an {\tt exec} call.
Our current implementation implements close-on-exec in the child for complete compatibility,
but a more security-sensitive application could easily implement ``close-on-fork'' semantics 
in the parent.
This clean division of labor retains full functionality
and facilitates extensibility.


\end{comment}

%\issue{1.3.b}{discuss alternative strategies of forking}


\subsection{Multi-process \syscall{execve}}




\subsection{Signaling and exit notification}




\subsection{System V IPC}

