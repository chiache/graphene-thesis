\section{Coordinating Guest OS States}
\label{sec:libos:namespaces}

%\fixmedp{RF: what are the few, powerful mechanisms?  Expect that there are many cases 
%with shared state; re-read this to see if it is clear how to generalize the approach}

%Recent library OS designs focus on single-process applications,
%which move a substantial portion of the OS APIs and state used by the application into the \libos{}.

%A key contribution of the \graphene{} 
%design is robust and flexible support for multi-process applications.
A multi-process application executes on \graphene{} 
with the abstraction that all of its processes runs on a single OS.
Each \thelibos{} instance services \linuxapis{}
from its local state whenever possible.
However, whenever a \thelibos{} instance must share a \libos{} state with other instances,
\thelibos{} has to coordinate the state across \picoprocs{}
via a RPC stream.
Within a sandbox, multiple \picoprocs{} 
can securely coordinate shared states of multi-process abstractions, including process IDs, exit notification and signaling, 
System V IPC mechanisms (message queues and semaphores), shared file system states, and shared file descriptor states (Table~\ref{tab:libos:multiproc}).
%Similar to previous \liboses{}~\cite{porter11drawbridge,baumann13bascule}, 
%\graphene{} uses the host file system; 
%the \libos{} implements file handles and translates between POSIX and the host ABI.
%Identifying the best division of labor for a \libos{} file system is 
%left for future work.
\thelibos{} contains a coordination framework
with several building blocks for implementing a shared multi-process abstraction.

\begin{table}
\footnotesize
\centering
\begin{tabular}{|p{.16\textwidth}|p{.20\textwidth}|p{.55\textwidth}|}
\hline
{\bf Ab\-strac\-tion} & {\bf Shared State} & {\bf Coordination Strategy} \\
\hline
Fork & 
\raggedright
PID namespace & Batch allocations of PIDs, children generally created using local state at parent.  \\
\hline
Signaling & PID mapping & Local signals call handler; remote signal delivery by RPC.  Cache mapping of PID to \picoproc{} ID. \\
\hline
\raggedright
Exit notification & 
\raggedright
Process status  & Exiting processes issue an RPC, or one synthesized if child becomes unavailable.  The {\tt wait} system call blocks until notification received by IPC helper. \\
\hline
{\tt /proc/[pid]} & Process metadata & Read over RPC.  \\
\hline
Message Queues & 
\raggedright
Key mapping \newline
Queued messages &
Mappings managed by a leader, contents stored in various \picoprocs{}.  When possible, send messages asynchronously, and migrate queues to the consumer.\\
\hline
Semaphores & 
\raggedright
Key mapping \newline
Semaphore count &
Mappings managed by leader, migrate ownership to \picoproc{} most frequently acquiring the semaphore. \\
\hline
\raggedright
File System & 
\raggedright
File truncate sizes \newline
Deleted files \newline
FIFO \& domain sockets \newline
Symbolic links
& No coordination; completely relying on \thehostabi{}; creating special files in the host to represent symbolic links. \\
\hline
\raggedright
Shared File Descriptors & 
\raggedright
Seek pointers & Mappings managed by parent, migrate ownership to \picoproc{} most frequently accessing the file descriptors. \\
\hline
\end{tabular}
\caption[Multi-process abstractions implemented in sysname{}]
{Multi-process abstractions implemented in \graphene{}, coordinated state, and implementation strategies.}
\label{tab:libos:multiproc}
\end{table}


%% Outline

% Basic idea of what we are doing
% Building blocks
% Implemented abstractions and examples (table)
%% Why not shared FDs?
% Optimizations/insights
% Why different from microkernels?



As an example of balancing security isolation and coordination APIs,
consider functionality that use the process ID namespace,
such as UNIX signaling or exit notification (e.g., {\tt waitpid()}).
In \graphene{}, the process ID namespace, 
as well as signaling and related system calls,
are implemented inside \thelibos{}.
A process can signal itself by having the \libos{} directly call the handler function.
When \picoprocs{} are in the same sandbox, they coordinate
to implement a consistent, shared process ID namespace,
as well as to send and receive signals amongst themselves.
Cross-process signals are implemented as RPCs
over kernel-managed streams.
When \picoprocs{} are in separate sandboxes,
they do not share a PID namespace, and cannot send signals to each other.
The reference monitor ensures that IPC abstractions, such as signaling,
cannot escape a sandbox by preventing the creation of kernel-level streams
across sandboxes.



%Figure~\ref{fig:libos:sandbox} illustrates several sandboxes with \picoprocs{}
%collaborating to implement a process ID namespace.  
%Because this namespace is a guest-level abstraction,
%different sandboxes can have overlapping process IDs, and
%cannot signal each other.
%If the connection between the two \picoprocs{} on the right of the figure
%is severed by subdividing the sandbox,
%the processes will become inaccessible to each other
%and each newly isolated library OS will treat the event as a process termination.
%%\fixmedp{See if we can get a better figure}

%dp:  Seems redundant, probably imported from S2
\begin{comment}
Within a sandbox, each library OS tracks the PIDs of other \picoprocs{}.
As children are created, each library OS updates its own replica of the 
process tree, with annotations for which host-level connection corresponds
to the remote process.  
If a \picoproc{} signals itself, the signal system call simply calls the 
appropriate signal handling function in the application. 
If a \picoproc{} signals another \picoproc{},
the signal essentially becomes an asynchronous remote procedure call
from the sending library OS to the receiving library OS.
Note that this is all transparent to the unmodified application.
Section~\ref{sec:graphene:namespaces} describes these library OS-internal
coordination mechanisms in more detail.
The current \graphene{} prototype supports 
a range of coordination abstractions, including signaling, 
exit notification, System V message queues, thread identifiers and groups, sessions,
and the process tree.
We believe this sample is sufficiently representative that
remaining tail of Linux IPC abstractions could be easily added.

The reference monitor ensures security isolation
simply by preventing \picoprocs{} in different sandboxes from 
sharing host-level streams.
We adopted this approach to maximize dynamic sandboxing flexibility,
rather than, say, attempt to multiplex one single library OS instance across multiple processes.
\end{comment}


A driving design insight is that the common case
for coordination is among pairs of processes.
Examples include a parent waiting on a child to exit, 
one process signaling another, or a single producer and single consumer
sharing a message queue.
Thus, \graphene{} optimizes for the common case of pairwise coordination,
reducing the overhead of replicating data (see Section~\ref{sec:libos:namespaces:lessons}).


%The rest of this section describes our coordination framework, 
%beginning with the coordination building blocks,
%and then explains the implementation of several multi-process abstractions.
%We conclude with lessons learned from optimizing  multi-process performance.

Although a straightforward implementation worked, tuning the performance was the most challenging aspect of the coordination framework. 
This section summarizes the lessons learned during the development of \graphene{}, from optimizing the coordination of various multi-process abstractions.
This section then
presents the design and driving insights of the coordination framework,
followed by representative examples 
and a discussion of failure recovery.

\subsection{Building blocks}
\label{sec:libos:namespaces:building-blocks}

The general problem underlying each of the coordinated \libos{} states is 
the coordination of {\bf namespaces}.  In other words, coordination between \picoprocs{} need 
a consistent mapping of names, such as process IDs or System V IPC resource keys, 
to the \picoproc{} implementing that particular item.  
Because many multi-process abstractions in Linux can also be used by single-process applications,
a key design goal is to seamlessly transition between single-process uses, serviced 
entirely from local \libos{} state, and multi-process cases, which coordinate shared abstractions over RPC.

%Picoprocesses then implement abstractions such as signals
%by issuing a remote procedure call (RPC) to the appropriate \picoproc{}.

\thelibos{} creates an {\bf IPC helper} thread within each \picoproc{}
to respond to coordination messages from other \picoprocs{}. 
%within the sandbox. %, using these broadcast and point-to-point streams.
An IPC helper
maintains a list of point-to-point RPC streams, and indefinitely waits for incoming messages.
For each multi-process abstractions coordinated over RPC,
\thelibos{} defines a protocol for formating the header of each message,
and determining the callback function for processing the message.
GNU Hurd~\cite{hurd} has a similar helper thread to implement signaling among a process's parent and
immediate children;
\graphene{} generalizes this design to share a broader range of multi-process abstractions among any \picoprocs{}.
IPC helpers in necessary 
in \thelibos{} to serve remote messages and receive responses atomically,
so one IPC helpers must be created in each \picoproc{}
after the application spawned its first child process.
To avoid deadlock among application threads and the IPC helper thread, 
an application thread may not both hold locks required by the helper thread to service an RPC request
and block on an RPC response from another \picoproc{}.
All RPC requests are handled from local state and do not issue recursive RPC messages.% \fixmedp{Check this}

Within a sandbox, all IPC helper threads exchange messages using a
combination of
a {\bf broadcast stream} for global coordination,
and {\bf point-to-point} RPC streams for pairwise interactions, 
minimizing overhead for unrelated operations.
The broadcast stream is created for the \picoproc{} as part of initialization.
Unlike other byte-granularity streams, the broadcast stream sends data at the granularity of messages,
to simplify the handling of concurrent writes to the stream.
Point-to-point RPC streams include the streams between parent and child processes established during \palcall{ProcessCreate},
and RPC streams created through connecting to a RPC server
identified by its URI.
% simply byte streams between two \picoprocs{};
%two processes may establish a point-to-point stream by passing handles through 
%an intermediate stream or over the broadcast stream.
%The handle-passing ABI is discussed further in Section~\ref{sec:graphene:impl}.
Because of the security isolation in the host,
only \picoprocs{} in the same sandbox can connect to each other through RPC.
%The broadcast stream is primarily used for failure 
%recovery (\S\ref{sec:namespaces:failurerecovery}); 
%Mmost operations use point-to-point streams to minimize overheads.
If a \picoproc{} leaves a sandbox to create a new one,
its broadcast stream is shutdown and replaced
with a new one, connected only between the \picoproc{} and any children created in the
new sandbox.

Because message exchange over the broadcast stream does not scale well,
we reduce the use of the broadcast stream to the minimum.
One occasion of using the broadcast stream is
{\bf \picoproc{} identifier allocation}.
Because each \picoproc{} needs an unique identifier to be recognized as a source or a destination of RPC messages, \thelibos{} generates a random number as the identifier of each \picoproc{} and confirms the uniqueness of the identifier over the broadcast stream.
Another occasion of using the broadcast stream
is {\bf leader recovery}, which happens when a namespace leader unexpectedly crashes
during coordination. The implementation of leader recovery will be discussed in Section~\ref{sec:libos:namespaces:details}.


For each namespace (e.g., process IDs, System V IPC resource keys), \thelibos{} elects one of the \picoprocs{} in a sandbox to serves as the {\bf leader}.
A leader is responsible for
managing and memorizing the allocation of identifiers or resources in a namespace,
in behave of all other \picoprocs{}.
For a namespace like the process ID namespace,
the leader subdivides the namespace for each \picoproc{} to reduce the RPC cost of allocation.
For example, the leader might allocate 50 process IDs to a \picoproc{} which intends to clone a new thread or process.
The \picoproc{} who receives the 50 process IDs becomes the {\bf owner},
and can further assign the process IDs
to children without involving the leader.
For a given identifier, the owner is the serialization point for all updates,
ensuring serializability and consistency for that resource.
%%% the leader's IPC helper has the added responsibility of coordinating 
%%% global state (name allocations).
%%% Our design minimizes the role of the leader, instead distributing responsibility 
%%% to specific \picoprocs{} when practical.  
%%% If the leader crashes, 
%%% a new leader can be elected. The detail of leadership recovery is discussed in
%%% Section~\ref{sec:namespaces:leader}. 
%More detailed discussion of the \graphene{}-internal
%RPC protocol is omitted for space;
%but it 
%consists of 30 message types which 
%encode both state replication and RPC 
%messages.

\subsection{Examples and discussion}
\label{sec:libos:namespaces:details}

%This subsection describes \libos{} coordination by example.
%\vspace{5pt}
\paragraph{Signals and exit notification.}
{\tt libLinux} implements signals
in various ways according to the causes of sigals.
For signals triggered by hardware exceptions (e.g., \code{SIGSEGV}),
\thelibos{} uses the hardware exception upcalls
in \thehostabi{}.
If a signal is sent from one of the processes for IPC purposes (e.g., \code{SIGUSR1}),
\thelibos{} exchanges RPC messages between \picoprocs{} to deliver the signal to the destination \picoproc{}.
%mplemented using a combination of 
%{\tt sigaction} data structures %adapted from Linux
%to track signal masks and pending signals;
%\pal{}-provided hardware exception upcalls (e.g., for {\tt SIGSEGV});
%and  RPCs for cross-\picoproc{} signals (e.g., for  {\tt SIGUSR1}).
If a process simply signals itself, {\tt libLinux} interrupts the targeted threads inside the process
and uses internal data structures
to call the appropriate user signal handler.
%this section describes how this local model is extended with RPCs for remote PIDs.
%For cross-process signals, we use the namespace coordination mechanism.
\thelibos{} implements all three of Linux's signaling namespaces:
process, process group, and thread IDs.
If a signal is sent for a process or a process group,
every threads within the process or the process group receives a copy of the signal,
even if the threads belong to different \picoprocs{}.


Exit notification in Linux is based on the same mechanism as signaling.
When a process exits, normally a \code{SIGCHLD} signal
is delivered from the child process to its parent,
to unblock the parent who might be waiting for exit notification using \syscall{wait} or \syscall{waitpid}.
Exit notification is always coordinated over RPC streams
between parent and child \picoprocs{}.



\begin{figure}
\centering
\includegraphics[width=\linewidth]{coordination.pdf}
\caption{Two pairs of \graphene{} \picoprocs{} in different sandboxes 
coordinate signaling and process ID management.
The location of each PID is tracked in {\tt libLinux}; Picoprocess 1 signals
\picoproc{} 2 by sending a signal RPC over stream 1,
and the signal is ultimately delivered using a 
library implementation of the {\tt sigaction} interface. Picoprocess 4 
waits on an {\tt exitnotify} RPC from  \picoproc{} 3 over stream 2. }
\label{fig:libos:coordination}
\end{figure}


Figure~\ref{fig:libos:coordination} illustrates two sandboxes with \picoprocs{}
collaborating to implement 
signaling and exit notification
within their own
process ID (PID) namespaces.  
Because process IDs and signals are \libos{} abstractions,
\picoprocs{} in 
each sandbox can have overlapping process IDs, and
cannot signal each other.
The host reference monitor ensures that
picoprocesses in different sandboxes cannot 
exchange RPC messages or otherwise communicate.
%If the connection between \picoprocs{} 1 and 2 is severed by subdividing the sandbox,
%the processes will become inaccessible to each other
%and each newly isolated library OS will treat the event as a process termination.



If \picoproc{} 1 (PID 1) sends a \code{SIGUSR1} to \picoproc{} 2 (PID 2), illustrated in Figure~\ref{fig:libos:coordination},
a \syscall{kill} call to \thelibos{} will check its cached mapping of PIDs to 
point-to-point streams.
If \thelibos{} cannot find a mapping, it may begin by sending a query to the leader
to find the owner of PID 2,
and then establish a coordination stream to \picoproc{} 2.
%The leader may also pass a point-to-point coordination stream handle.
Once this stream is established, \picoproc{} 1 can send a  
signal RPC to \picoproc{} 2 (PID 2).
When \picoproc{} 2 receives this RPC, 
\thelibos{} will then query its local \code{sigaction} 
structure and mark \code{SIGUSR1} as pending.
The next time \picoproc{} 2 calls \syscall{kill},
the \code{SIGUSR1} handler will be called upon return. Also in Figure~\ref{fig:libos:coordination}, \picoproc{} 4 (PID 2) waits on 
\picoproc{} 3 termination (in the same sandbox with PID 1). When \picoproc{} 3 terminates, it invokes the library implementation of exit, which issues
an \code{exitnotify} RPC to \picoproc{} 4.
%In this example, \picoprocs{} in different sandboxes have the same PID number, but this does not cause conflict as they are isolated and can only communicate with processes in the same sandbox.

%%% Using a helper thread alongside each process to asynchro-
%%% nously handle signaling
%%% is common in many \microkernel{}-based OSes such as GNU Hurd~\cite{hurd}.
%%% GNU Hurd also maintains local mappings of PIDs and RPC ports for inter-process messaging.
%%% However, PID namespace in GNU Hurd is only tracked between the parents/children, so signaling cannot be transferred between arbitrary processes.
%%% \graphene{} handles PID namespace with global consistency inside a sandbox,
%%% but requires no intensive RPCs to any centralized service.   

The signaling semantics of \thelibos{} closely match the Linux behavior, which
delivers signals upon returning from a system call
or an exception handler.
Each process and thread have \code{sigaction} structures from the Linux source 
that implement the
POSIX specification, including handler functions, as well as masking signals and
reentrant behavior.
\thelibos{} does not modify \libc{}'s signal handling code. % is unmodified on \graphene{}.
%We extend the \pal{} with ABIs for explicit upcalls on certain hardware exceptions, such
%as divide-by-zero or segmentation faults.
%Signals from other processes, such as {\tt SIGUSR1}, are generally delivered upon 
%return from a call into {\tt libLinux}; 
If an application has a signal pending for too long,
e.g., the application is in a CPU-intensive loop, {\tt libLinux} can use \palcall{ThreadInterrupt} in \thehostabi{} to interrupt the running thread. 


% Daniela Oliveira commented. old caption for coordination
%\caption{\graphene{} namespace coordination example.  
 % Two applications and {\tt libLinux} instances
 % coordinate signaling and process ID management.
 % The location of each PID is tracked in {\tt libLinux};
 % a signal message is sent over a host stream
 % using the {\tt action} message, 
 % and the signal is ultimately delivered using a 
 % library implementation of the {\tt sigaction} interface.
%}


\begin{comment}
\graphene{} internally indexes point-to-point handles using PIDs.
In order to facilitate reallocation of PIDs without global coordination, 
\graphene{}-internal PIDs also include a {\em generation number},
allowing \picoprocs{} to lazily detect reuse similar to generation numbers 
for inodes in NFS~\cite{sandberg85nfs}.
\end{comment}

\paragraph{System V IPC.} System V IPC
maps an application-specified key onto a unique identifier.
All System V IPC abstractions, including message queues and semaphores,
are then referenced by a resource ID, which is arbitrarily allocated.
Similar to process IDs, 
%In order to service IPC requests, including identifier creation, from local state,
the leader divides the namespace of resource IDs among the \picoprocs{},
so that any \picoproc{}
can allocate a resource ID from local state instead of involving the leader. %a pre-allocated range.
Unlike the resouce IDs, System V IPC keys must be centrally managed
by the leader,
since an application might autonomously assign System V IPC keys to its processes.
%The leader also dynamically allocates keys to \picoprocs{}.
Global coordination is required to ensure that the same key maps to the same resource ID;
the leader caches this information, but the owner of the resource ID 
makes the definitive decision about whether a ID mapping is still valid.
A key which does not have a valid mapping can be assigned to a resource ID by any \picoproc{} to allocate a {\em private} IPC resource.



\paragraph{System V IPC message queues.} In \graphene{}, the owner of a queue ID is responsible for 
storing the messages written to a System V IPC message queue.
To ensure the serializability and consistency of all messages, any delivery and reception of messages must go through the owner.  
In the initial implementation of \thelibos{}, sending or receiving messages remotely over a RPC stream
orders of magnitude slower than accessing a local message queue.
This observation led to two essential optimizations.  
First, sending to a remote
message queue was made asynchronous.  In the common case, the sender can simply assume 
the send succeeded, as the existence and location of the queue have already been determined.
The only risk of failure arises when another process deletes the queue.
When a queue is deleted, the owner sends a deletion notification to all other \picoprocs{}
that previously accessed the queue.
If a pending message was sent concurrently with the deletion notification 
(i.e., there is an application-level race condition), 
the message is treated as if it were sent after the deletion and thus dropped.
The second optimization migrates queue ownership from the producer to the consumer,
which must read queue contents synchronously.

%\fixmedp{bill: it is referenced in the Semaphores section, but here there isn't any talk about migrating msg queues to the most active \picoproc{}. also, the section starts with ``two essential optimizations. First \ldots'', but there isn't a second. is queue ownership migration the second?}

Because non-concurrent processes can share a message queue,
our implementation also uses a common file naming scheme to serialize message queues to disk.
If a \picoproc{} which owns a message queue exits, 
any pending messages are serialized to a file in the host,
and the receiving process may regain the ownership of the message queue later from the leader and recover the serialized messages.
%In order to prevent happens-before violations in case of failure, 
%message queues may be checkpointed more aggressively.


%\paragraph{Failure Recovery.}
%\label{sec:namespaces:failurerecovery}
%The \graphene{} coordination protocols are designed such that the leader does not store any unrecoverable information---the leader
%only caches the current name allocations.
%Because we assume that \picoprocs{} within a sandbox trust each other, 
%a new leader can simply broadcast a request to 
%recreate the current name allocations.  
%If the leader crashes, a simple leader election protocol is sufficient, e.g., picking the smallest live PID.

%\daniela{I wonder if the remainder of this section should be part of implementation details(section 5)}

\paragraph{System V IPC semaphores.} System V IPC semaphores 
follow a similar pattern to message queues.
Each semaphore is owned by a \picoproc{};
a semaphore can be directly accessed by its owner as a local state,
whereas other \picoprocs{} all have to access the semaphore through the owner over RPC.
Since a semaphore shares the same performance pattern as a message queue,
\thelibos{} applies the same optimization
of migrating the ownership of a semaphore
%, where ownership of a given semaphore is migrated
to the \picoproc{} that most frequently acquires the semaphore.
%If the owner of a semaphore exits, \graphene{} transfers ownership to the leader.  rather than serialize the semaphore to disk.
Another optimization of message queues, by making the updates asynchronous,
does not apply to semaphores,
because a participating \picoproc{} cannot proceed before successfully acquiring the semaphore. 
Most of the overhead in the Apache benchmark (see Section~\ref{sec:eval:graphene}) is attributable to semaphore overheads.
%and, in ongoing work, we will likely optimize this by 
%either expanding the host ABI to share synchronization primitives within a sandbox,
%using shared memory to reduce semaphore latency.

\paragraph{Shared file descriptors.}
The seek pointer of each file descriptor is implemented as a \libos{} abstraction;
when reading or writing to a host file,
\thehostabi{} always obtains an absolute pointer from the beginning of the file.
Although most applications
do not share the seek pointer of an inherited file descriptor among processes,
the {\tt clone} system call
can can be called with the {\tt CLONE\_FILES} flag
and create a process
which shares the whole file descriptor table with its parent.
%The default Linux behavior is that children copy the open handles and file seek cursors,
%but subsequent cursor movements are not shared between parent and child.
%Shared file descriptor table can be requested by passing the {\tt CLONE\_FILES} flag to the {\tt clone} system call.
%Any new file descriptor opened in a shared table will be visible by every process cloned in this way, as well as subsequent cursor update.
%None of our target applications have required a shared seek cursor, and it is not currently implemented,
%but would be a straightforward extension to current RPC mechanisms.
%We expect that the current RPC mechanisms could easily be extended to synchronize a seek pointer among \picoprocs{}.
To share a file descriptor table among \picoprocs{},
one of \picoprocs{} (usually the oldest one)
must be the leader of the file descriptor table to manage all mappings
from file descriptors to the child \picoproc{} who owns the state of the file descriptors including the seek pointers.
Every updates to a seek pointer must goes through the owner of the file descriptor (not the leader).
The migration-based optimization for System V IPC message queues and semaphores is also effective for optimizing the performance of shared file descriptors. 

\paragraph{Shared file system states.}
A chroot file system in \thelibos{}
is restricted by \thehostabi{} to externalize any file system states.
Other shared file system states are implemented as \libos{} abstractions, and have to be coordinated among \picoprocs{}.
For example, a POSIX file system can contain special files such as a FIFO (first-in-first-out); a path bound to a UNIX domain socket;
a symbolic link; or a file system lock.
Implementation of these special files cannot
completely depend on \thehostabi{},
since the abstractions of \thehostabi{} contains only regular files and directories.

%Coordinating these states can be cause significant slowdown on regular file system operations, so we simply export the state in regular files on the host,
% and atomically update them by renaming.

A simple approach to coordinating file system states is to share a ``dummy'' host file.
For example,  \thelibos{} can store the target of a symbolic link
in a regular file on the chroot file system.
For a FIFO, a bounded UNIX domain socket, or a file system lock,
\thelibos{} can store a mapping to the corresponding RPC stream, or to the \picoproc{} which owns the abstraction.
By using the host file system as a less efficient but permanent coordination medium,
\thelibos{} can extend the coordination framework for sharing file system states.



\paragraph{Shared memory.} The \graphene{} architecture 
does not currently permit shared memory among \picoprocs{}.
This thesis expects that an extra \hostapi{} and the existing support for System V IPC coordination would be sufficient to implement this,
with the caveat that the host must be able to handle sandbox disconnection gracefully, perhaps converting the pages to copy-on-write.
Thus far \graphene{} have avoided the use of shared memory in \thelibos{},
both to maximize flexibility in placement of \picoprocs{}, potentially on an unconventional host (e.g., Intel SGX) or different physical machines.
and to keep all coordination requests explicit.
Shared memory may also be useful to reduce latency for RPC messaging among \picoprocs{} when 
all \picoprocs{} are on the same host.


%in \graphene{} are implemented by a simple producer-consumer model.
%%% Without blocking, the latency of an inter-process semaphore operation equals to a round-trip of RPC messages.
%%% We observed that IPC semaphores can benefit from the same optimization
%%% used by message queues,
%%% based on asynchronous sending.
%%% However, when non-concurrent processes share a semaphore, it does not worth serializing the semaphore state to disk.
%%% When the owner of a semaphore exits,
%%% the semaphore state will be migrated to the leader,
%%% if it has a non-zero counter.
%%% IPC semaphores are intensively used in Apache web servers with a multi-process model. 

 


\paragraph{Failure and disconnection tolerance.}  
\thelibos{} must tolerate disconnection between collaborating \picoprocs{},
either because of crashes or blocked RPC streams.  In general, \thelibos{} makes 
these disconnections isomorphic to a reasonable application behavior,
although there may be some edge cases that cannot be made completely transparent to the application.

In the absence of crash recovery, placing shared state in a given \picoproc{} introduces the risk that an errant 
application will corrupt shared \libos{} state.  The \microkernel{} approach of 
moving all shared state into a separate server process is more resilient to this problem.
Anecdotally, \thelibos{}'s performance optimization of migrating ownership to the process that 
most heavily uses a given shared abstraction also improves the likelihood that only the corrupted
process will be affected.  
Making each \thelibos{} instance resilient to arbitrary memory corruption of any \picoproc{} is left for future work.


\paragraph{Leader recovery.}
\thelibos{} provides a leadership recovery mechanism when a leader failure is detected.
A non-leader \picoproc{} can detect the failure of a leader by either observing the shutdown of RPC streams or timing out on waiting for responses. 
Once the \picoproc{} detects leader failure, \thelibos{} sends out a message on the broadcast stream to volunteer for claiming the leadership.
After a few rounds of competition, the winning \picoproc{} becomes the new leader and recover the namespace state by reading a namespace snapshot stored before the crash of the former leader
or recollecting from other \picoprocs{} in the same sandbox.

%After a \picoproc{} being elected as the leader,
%the leader state,
%including all the allocated IDs and the RPC stream addresses,
%must be recovered. 
%Recollecting the leader state from all the \picoprocs{} is possible
%but can be inefficient,
%given the new leader may not have knowledge about every \picoproc{}.
%To simplify the implementation,
%we make the leaders of a namespace periodically serialize their states to disk for later recovery.

When a \picoproc{} is moved to a new sandbox, \thelibos{} will naturally detect the failure of leader because of blocked RPC. % streams are closed by the reference monitor.
The sandboxed \picoproc{} will be the only candidate for leadership because the host has replaced the broadcast stream;
as a result, the sandboxed \picoproc{} seamlessly transitions to new namespaces isolated from the previous sandbox.

%Once it starts the leadership recovery, it will automatically win because no other \picoproc{} is sharing the broadcast stream. The procedure can be skipped by informing the sandboxed \picoproc{} before detaching.

\subsection{Lessons learned}
\label{sec:libos:namespaces:lessons}

The current coordination design is the product of several iterations, which began 
with a fairly simple RPC-based implementation. %, and was then refined based on profiling.
This subsection summarizes the design principles that have emerged from this process.
%We present high-level facets of the design along with the insight
%behind the decision.  The next subsections synthesize these aspects 
%with specific examples of signaling and message queues.

\paragraph{Service requests from local state whenever possible.}
Sending RPC messages over Linux pipes is expensive;
this is unsurprising, given the long history of 
work on reducing IPC overhead in microkernels~\cite{liedtke93sosp,chen93memory}.  
We expect that \graphene{} performance could be improved on a 
\microkernel{} with
a more optimized IPC substrate, such as L4~\cite{liedtke95sosp,klein09sel4,elphinstone13microkernels};
we take a complementary approach of avoiding IPC if possible.
%but this is beyond the scope of our work, and we want \graphene{} to perform well on any 
%host OS.

An example of this principle is migrating message queues to the ``consumer'' when a 
clear producer/consumer pattern is detected, or migrating semaphores to the most frequent requester.
In these situations, synchronous RPC requests can be replaced with local function calls, improving
performance substantially.  For instance, migrating ownership of message queues 
reduced overhead for message receive by a factor of $10\times$.

\paragraph{Lazy discovery and caching improve performance.}  
No library OS keeps a complete replica of all distributed state,
avoiding substantial overheads to pass messages replicating irrelevant state.
Instead, \graphene{} incurs the overhead of discovering the owner of a name
on the first use, and amortizes this cost over subsequent uses.
Part of this overhead is potentially establishing a point-to-point stream,
which can then be cached for subsequent use.
For instance, the first time a process sends a signal, the helper thread 
must figure out whether the process id exists, to which \picoproc{} it maps,
and establish a point-to-point stream to the \picoproc{}.
If they exchange a second signal, the mapping is cached and reused, amortizing this 
setup cost.  For instance, the first signal a process sends to a new processes
takes \roughly{}2ms, but subsequent signals take only \roughly{}55 \us{}.

\paragraph{Batched allocation of names minimizes leader workload.}
In order to keep the leader off of the critical path of operations like {\tt fork}, 
the leader typically allocates larger blocks of names, such as process IDs or System V queue IDs.
In the case of \syscall{fork}, if a \picoproc{} creates a child, it will request a batch of 
PIDs from the leader (50 by default).  Subsequent child PID allocations will be made from the same 
batch without consulting the leader.
Collaborating processes also cache the owner of a range of PIDs, avoiding 
leader queries for adjacent queries.

%% dp: Sad to see this go, but it is sort of subsumed by the other discussion
\paragraph{The coordination within a sandbox is often pairwise.}
\graphene{} optimizes the common case of pairwise coordination,
by authorizing one side of the coordination to dictate the abstraction state,
but also allows
more than two processes to share an abstraction.
Based on this insight, 
we observe that {\em not all shared state need be replicated by all \picoprocs{}}.
Instead, we adopt a design where one \picoproc{} is authoritative for a given name (e.g., a process ID or a System V queue ID).
For instance, all possible thread IDs are divided among the collaborating \picoprocs{},
and the authoritative \picoproc{} either responds to RPC requests for this thread ID (e.g., a signal)
or indicates that the thread does not exist.
This trade does make commands like ``\code{ps}'' slower, 
but optimizes more common patterns, such as waiting for a child to exit.

\paragraph{Make RPCs asynchronous whenever possible.} 
For operations that must write to state in another \picoproc{}, 
the \graphene{} design strives to cache enough information in the sender 
to evaluate whether the operation will succeed, thereby obviating the 
need to block on the response.  This principle is applied to lower the overheads
of sending messages to a remote queue.


%this is a mapping to a thread within a specific \picoproc{}; for a message queue key,
%the mapping might be empty if the queue does not exist, or it may point to the \picoproc{} storing the pending messages.
%% The general problem underlying each of these coordination APIs is 
%% {\bf namespace management}.  In other words, coordinating \picoprocs{} need a consistent mapping
%% of names, such as a thread ID or System V message queue ID, to the authoritative \picoproc{} for that abstraction, if one exists.


\paragraph{Summary.}
The current \graphene{} design minimizes the use of RPC,
avoiding heavy communication overheads in the common case.
This design also allows for substantial flexibility to dynamically moving processes out of
a sandbox.  Finally, applications do not need to select different 
library OSes {\em a priori} based on whether they are multi-process or single-process---\graphene{}
automatically uses optimal single-process code until otherwise required.
