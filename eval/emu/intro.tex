This section evaluates the performance
of Linux ABI emulation
inside the \graphene{} \libos{},
or \thelibos{}.
The evaluation is based on benchmark results
of Linux system call inside a \picoproc{},
since the Linux system call table is the main target that \thelibos{} emulates
as a guest.
The Linux system call performance
on \thelibos{}
is affected by the emulation strategies
or the underlying host abstractions (i.e., \hostapis{})
that \thelibos{}
selects to implement system call.
This section
shows the emulation overhead of \thelibos{}
by comparing the benchmark results
of Linux system call in a \picoproc{} with the native system call performance in a Linux process
and the related \hostapis{}, as evaluated in Section~\ref{sec:eval:pal}.
Finally,
the evaluation shows that with the right emulation strategies,
\thelibos{} can achieve acceptable performance on system calls.


The benchmark results in this section are based on
\lmbenchwithver{}~\cite{McVoy:lmbench},
extended with extra tests.
Each result
reports a mean and 95\% confidence interval,
assuming the benchmark results are normally distributed;
to improve the precision,
the number of iterations in each test is increased to at least a thousand times, which is effectively lower the variance
in most tests.
Although assuming a normal distribution may not be realistic for most benchmark results,
the error is likely to be marginal with the very low variance
observed in the tests.
%we use the default number of iterations for each test case.
%We have added code to \lmbench{} to also calculate 95\% confidence intervals 
%within a run~\footnote{The lmbench authors deliberately exclude variation statistics
%because most methods assume a known distribution, generally a normal distribution---an 
%assumption which is often not the case for a computer microbenchmark~\cite{staelin05lmbench}.
%Though confidence intervals should be taken with a grain of salt, 
%we include them because they clearly indicate that these experiments have very low variance. In 
%a few cases of minor performance improvement, one can assess the impact of noise.}.
Besides, to measure the marginal cost of the \seccomp{} filter and reference monitor on a Linux host,
the experiments include the cases both with
and without the \seccomp{} filter and reference monitor.


The evaluation results categorize
system calls as two types.
The first type of system calls can be completely serviced inside \thelibos{};
in this case, the latency of system calls
inside \graphene{} or \graphenesgx{}
is close to the native performance,
or even faster than native,
depending on how optimized the emulation is.
\graphene{} emulates
these system calls with relative small costs or even performance gains
because there is almost no needs
to use system calls in the host OS or even exit enclaves.
%the evaluation results show that these system calls are even faster than native, because \thelibos{} does not switch context into the kernal space
%to service the system calls.
For instance,
a system call which only fetches a in-kernel variable, such as \syscall{getppid},
is 75\% faster in \graphene{} or \graphenesgx{} than native.
%The second type of system call requires translation to a native host system calls.
%For instance, 
%the self-signaling test (sig overhead)
%just calls the signal handler as a function,
%which is almost twice as fast
%as the Linux kernel implementation.
