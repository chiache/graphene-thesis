\papersubsection{Network sockets and pipes}
\label{sec:eval:libos:streams}


\begin{table}[t!b!]
\input{tables/lmbench-streams}
\caption{Network socket and pipe performance based on \lmbench{}. Comparison is among (1) native Linux processes; (2) \graphene{} on Linux host, both without and with \seccomp{} filter ({\bf +SC}) and reference monitor ({\bf +RM}); (3) \graphenesgx{}.
System call latency is in microseconds, and lower is better.
System call bandwidth is in megabytes per second, and higher is better. 
%The file system is measured in thousands operations per second, and higher is better.
Overheads are relative to Linux; negative overheads indicate improvement.} 
\label{tab:eval:libos:lmbench-streams}
\end{table}


Figure~\ref{tab:eval:libos:lmbench-streams}
lists the latency and bandwidth of pipes, UNIX domain sockets, UDP sockets, and TCP sockets.
Based on the latency of transmitting single-byte messages, 
\graphene{} causes at most 30--85\% overheads
on all types of I/O streams.
pipes and UNIX domain sockets have similar latency and throughput since \thelibos{} uses the same host abstractions (i.e., RPC streams) to emulate both.

 
