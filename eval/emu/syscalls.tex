\papersubsection{Single-process system calls}
\label{sec:eval:libos:syscalls}


\begin{table}[t!b!]
\input{tables/lmbench-syscalls}
\caption{Single-process system call performance based on \lmbench{}. Comparison is among (1) native Linux processes; (2) \graphene{} on Linux host, both without and with \seccomp{} filter ({\bf +SC}) and reference monitor ({\bf +RM}); (3) \graphenesgx{}.
System call latency is in microseconds, and lower is better.
Overheads are relative to Linux; negative overheads indicate improvement.} 
\label{tab:eval:libos:lmbench-syscalls}
\end{table}


System calls directly serviced
inside \thelibos{}
tend to have a close-to-native performance
or even gain significant speed-up.
Table~\ref{tab:eval:libos:lmbench-syscalls}
lists several system call samples that share the characteristic,
including \syscall{getppid},
\syscall{sigaction}, and common file operations on a pseudo device such as \code{/dev/zero}.
The benchmark results on these system calls
show at most 20--80\% speed-up
than native performance.
File operations on \code{/dev/zero} have
slightly higher overheads,
especially for \syscall{open} and \syscall{stat}
which suffer the cost
of directory cache lookup.
%The evaluation observes the same performance
%on different hosts, either with or without
%security checks.
Even if a host implements \thehostabi{} with significant overheads,
such as in an \sgx{} enclave,
these system calls have a similar performance because no \hostapi{} is involved.




%and several cases where the host OS delivers an exception
%(segmentation faults or illegal system calls).
%A few system calls show optimized performance
%in \graphene{} and \graphenesgx{} compared to Linux.
%For instance,
%\syscall{getppid}, \syscall{sigaction}, and sending a \code{SIGUSR1} signal (to current process)
%are 23--80\% faster in \graphene{} and \graphenesgx{} than in a native Linux process,
%due to no native host system calls and little emulation complexity.



%As another example, access to a pseudo file
%such as \code{/dev/zero}
%is also a feature emulated
%inside \thelibos{},
%with less speedups or slowdowns.
%Opening \code{/dev/zero}
%or retrieving the attributes of \code{/dev/zero} using \syscall{stat}
%is 21--29\% slower in \graphene{} or \graphenesgx{} than native,
%due to file system directory cache lookup latency
%in \thelibos{}.
%For other system calls, \thelibos{} emulates \syscall{fstat} and \syscall{read}
%on \code{/dev/zero}
%with negligible overheads,
%and emulates \syscall{write} with
%similar latency as \syscall{read}.
%For each of these system calls, either the \seccomp{} filter, reference monitor, or enclave
%has no impact on the performance.


Other two benchmark results in Table~\ref{tab:eval:libos:lmbench-syscalls}
show significant overheads
%on \thelibos{}.
on delivering hardware and OS-triggered exceptions.
One example
is the overheads on system calls directly made from a statically compiled binary
using \assembly{syscall} or \assembly{int \$80} instructions.
Using host-specific system call restriction,
such as a \seccomp{} filter
on a Linux host,
\graphene{} can capture these system calls %direct made from application binaries
and redirect them back to \thelibos{} as an exception. % (not supported if \seccomp{} filter is disabled).
%For \graphenesgx{},
%the SGX hardware also captures these system calls
%inside an enclave
%and delivers an
%``illegal instruction'' exception.
%\graphene{} services these direct system calls primarily as a compatibility feature;
Base overhead on a direct system call
is up to 24\x{} on the Linux host, or 128\x{} on SGX.
Another result shows the overhead on
sending a \code{SIGSEGV} signal
to be around 3\x{} and 15\x{} for Linux and SGX hosts, respectively.
%A potential optimization
%is to virtualize the exception handler using hardware support such as VT~\cite{VT} so that a \libos{} can avoid trapping
%into the host OS
%at exceptions.















