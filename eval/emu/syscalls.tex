\papersubsection{Single-process system calls}
\label{sec:eval:libos:syscalls}


\begin{table}[t!b!]
\input{tables/lmbench-syscalls}
\caption{Single-process system call performance based on \lmbench{}. Comparison is among (1) native Linux processes; (2) \graphene{} on Linux host, both without and with \seccomp{} filter ({\bf +SC}) and reference monitor ({\bf +RM}); (3) \graphenesgx{}.
System call latency is in microseconds, and lower is better.
Overheads are relative to Linux; negative overheads indicate improvement.} 
\label{tab:eval:libos:lmbench-syscalls}
\end{table}


System calls directly serviced
inside an \thelibos{} instance tends to show performance
close to native or even better.
Figure~\ref{tab:eval:libos:lmbench-syscalls}
lists the latency of several of these system calls,
including \syscall{getppid},
\syscall{sigaction}, and accessing a pseudo device such as \code{/dev/zero}.
The benchmark results on these system calls
show at most 20--80\% speedups
than the native performance.
Accessing \code{/dev/zero} has slightly higher latency
due to internal cost of looking up
the path in directory cache.
The performance of these system calls show no difference
on each host target, either with or without security enforcements.

%and several cases where the host OS delivers an exception
%(segmentation faults or illegal system calls).
%A few system calls show optimized performance
%in \graphene{} and \graphenesgx{} compared to Linux.
%For instance,
%\syscall{getppid}, \syscall{sigaction}, and sending a \code{SIGUSR1} signal (to current process)
%are 23--80\% faster in \graphene{} and \graphenesgx{} than in a native Linux process,
%due to no native host system calls and little emulation complexity.



%As another example, access to a pseudo file
%such as \code{/dev/zero}
%is also a feature emulated
%inside \thelibos{},
%with less speedups or slowdowns.
%Opening \code{/dev/zero}
%or retrieving the attributes of \code{/dev/zero} using \syscall{stat}
%is 21--29\% slower in \graphene{} or \graphenesgx{} than native,
%due to file system directory cache lookup latency
%in \thelibos{}.
%For other system calls, \thelibos{} emulates \syscall{fstat} and \syscall{read}
%on \code{/dev/zero}
%with negligible overheads,
%and emulates \syscall{write} with
%similar latency as \syscall{read}.
%For each of these system calls, either the \seccomp{} filter, reference monitor, or enclave
%has no impact on the performance.


Two other benchmark results in Figure~\ref{tab:eval:libos:lmbench-syscalls}
show the overheads
of delivering hardware and OS-triggered exceptions.
One example
is the overheads on system calls directly made from a statically compiled binary,
using \assembly{syscall} or \assembly{int \$80} instructions.
Using host-specific system call restriction,
such as a \seccomp{} filter
on a Linux host,
\graphene{} can capture these system calls %direct made from application binaries
and redirect them back to \thelibos{} as an exception. % (not supported if \seccomp{} filter is disabled).
%For \graphenesgx{},
%the SGX hardware also captures these system calls
%inside an enclave
%and delivers an
%``illegal instruction'' exception.
%\graphene{} services these direct system calls primarily as a compatibility feature;
The based overheads on direct system calls
are up to 24\x{} on the Linux host, and 128\x{} on SGX.
As another example,
delivering a memory fault
into a \picoproc{} and an enclave has 3\x{} and 15\x{} overheads, respectively.
%A potential optimization
%is to virtualize the exception handler using hardware support such as VT~\cite{VT} so that a \libos{} can avoid trapping
%into the host OS
%at exceptions.















